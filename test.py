import os
import argparse


from isaaclab.app import AppLauncher
os.environ["ENABLE_CAMERAS"] = "1"

# 1. å¯åŠ¨ Isaac Sim (å¿…é¡»åœ¨å…¶ä»– import ä¹‹å‰)
parser = argparse.ArgumentParser(description="Evaluate VLD Policy in Isaac Sim.")
parser.add_argument("--robot", type=str, default="franka_panda", help="Name of the robot.")
parser.add_argument("--num_envs", type=int, default=1, help="Number of environments.")
AppLauncher.add_app_launcher_args(parser)
args_cli = parser.parse_args()
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# ---------------------------------------------------------------------------
# Standard Imports
# ---------------------------------------------------------------------------
import torch
import numpy as np
import hydra
import json
import math
from omegaconf import DictConfig
from copy import deepcopy
from IPython import embed
from scipy.spatial.transform import Rotation
import random

# Isaac Lab Imports
import isaaclab.sim as sim_utils
from isaaclab.scene import InteractiveScene, InteractiveSceneCfg
from isaaclab.sensors import Camera, CameraCfg
from isaaclab.assets import AssetBaseCfg, ArticulationCfg
from isaaclab.actuators import ImplicitActuatorCfg
from isaaclab.controllers import DifferentialIKController, DifferentialIKControllerCfg
from isaaclab.managers import SceneEntityCfg
from isaaclab.utils import convert_dict_to_backend
from isaaclab_assets import FRANKA_PANDA_HIGH_PD_CFG, UR10_CFG
from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR
from isaaclab.utils import configclass
from isaaclab.markers.config import FRAME_MARKER_CFG
from isaaclab.markers import VisualizationMarkers, VisualizationMarkersCfg
from isaaclab.utils.math import subtract_frame_transforms
import isaacsim.core.utils.prims as prim_utils

# VLD Model Imports (å‡è®¾è¿™äº›æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹)
from model.main import VLDDiffusionSystem
from transformers import BertTokenizer

# ---------------------------------------------------------------------------
# Scene Configuration (å¤ç”¨ isaac_render.py çš„é…ç½®)
# ---------------------------------------------------------------------------
TARGET_SCALE = 0.2
TARGET_INIT_POS = (0.5, 0.0, 0.2)
TARGET_INIT_ROT = (0.7, 0.0, 0.7, 0.0)

# è¿™é‡Œä½ éœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ USD è·¯å¾„
MY_NEW_OBJECT_CFG = ArticulationCfg(
    spawn=sim_utils.UsdFileCfg(
        usd_path="/home/hhhar/liuliu/vld/assets/articulation/100392/mobility_annotation_gapartnet/mobility_annotation_gapartnet.usd",
        rigid_props=sim_utils.RigidBodyPropertiesCfg(disable_gravity=False),
        scale=[TARGET_SCALE for _ in range(3)],
        semantic_tags=[("class", "target")],
    ),
    init_state=ArticulationCfg.InitialStateCfg(pos=TARGET_INIT_POS, rot=TARGET_INIT_ROT),
    actuators={
        "all_joints": ImplicitActuatorCfg(
            joint_names_expr=["joint_.*"],
            effort_limit=400.0,
            velocity_limit=100.0,
            stiffness=0.0,
            damping=10.0,
        )
    }
)

bbox_pts_cfg = VisualizationMarkersCfg(
    prim_path="/World/Visuals/testMarkers",
    markers={
        "marker1": sim_utils.SphereCfg(
            radius=0.005,
            visual_material=sim_utils.PreviewSurfaceCfg(
                diffuse_color=(1.0, 0.0, 0.0)),
        ), })

@configclass
class TableTopSceneCfg(InteractiveSceneCfg):
    ground = AssetBaseCfg(
        prim_path="/World/defaultGroundPlane",
        spawn=sim_utils.GroundPlaneCfg(),
        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, -1.05)),
    )
    dome_light = AssetBaseCfg(
        prim_path="/World/Light", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))
    )
    table = AssetBaseCfg(
        prim_path="{ENV_REGEX_NS}/Table",
        spawn=sim_utils.UsdFileCfg(
            usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Mounts/Stand/stand_instanceable.usd", scale=(2.0, 2.0, 2.0)
        ),
    )
    
    if args_cli.robot == "franka_panda":
        robot = FRANKA_PANDA_HIGH_PD_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
    elif args_cli.robot == "ur10":
        robot = UR10_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
    
    robot.spawn.semantic_tags = [("class", "robot")]
    my_new_articulation = MY_NEW_OBJECT_CFG.replace(prim_path="{ENV_REGEX_NS}/MyNewObject")

def quat_to_rotmat(q):
    """
    q: array-like [w, x, y, z]
    returns 3x3 rotation matrix
    """
    q = np.asarray(q, dtype=float)
    if q.shape != (4,):
        q = q.ravel()[:4]
    # normalize to be safe
    q = q / np.linalg.norm(q)
    w, x, y, z = q
    # 3x3 rotation matrix (Hamilton convention)
    R = np.array([
        [1 - 2*(y*y + z*z),     2*(x*y - z*w),     2*(x*z + y*w)],
        [2*(x*y + z*w), 1 - 2*(x*x + z*z),     2*(y*z - x*w)],
        [2*(x*z - y*w),     2*(y*z + x*w), 1 - 2*(x*x + y*y)]
    ], dtype=float)
    return R

def transform_points_by_pose(points, p, q, scale=1.0):
    """
    points: (N,3) np.array in local coordinates
    p: (3,) translation in world coords
    q: [w,x,y,z] quaternion (world orientation of the local frame)
    scale: scalar or (3,) array, applied in local frame before rotation/translation
    returns transformed_points: (N,3)
    """
    pts = np.asarray(points, dtype=float).reshape(-1, 3)
    # apply local scale (support scalar or per-axis)
    scale = np.asarray(scale, dtype=float)
    if scale.size == 1:
        pts = pts * float(scale)
    else:
        pts = pts * scale.reshape(1, 3)
    # rotate
    R = quat_to_rotmat(q)
    pts_rot = pts.dot(R.T)   # (N,3)
    # translate
    return pts_rot + np.asarray(p).reshape(1, 3)



# ---------------------------------------------------------------------------
# Inference Agent (Core Logic)
# ---------------------------------------------------------------------------
class VLDInferenceAgent:
    def __init__(self, cfg, device):
        self.cfg = cfg
        self.device = device
        self.scaled_bbox = []
        self.action_type = []
        self.bbox_info = []
        self.action_queue = [] 
        
        # 1. åŠ è½½ç»Ÿè®¡é‡ (Min/Max) ç”¨äºåå½’ä¸€åŒ–
        print(f"Loading stats from {cfg.path.action_stats_path}")
        with open(cfg.path.action_stats_path, 'r') as f:
            stats = json.load(f)
        self.min_val = torch.tensor(stats['min'], device=device)
        self.max_val = torch.tensor(stats['max'], device=device)
        self.bins = torch.linspace(-1, 1, cfg.model.num_action_bins, device=device)

        # 2. åˆå§‹åŒ–æ¨¡å‹
        print("Initializing Model...")
        self.model = VLDDiffusionSystem(**cfg.model)
        self.tokenizer = BertTokenizer.from_pretrained(cfg.model.text_model_name)
        
        # 3. åŠ è½½æƒé‡
        print(f"Loading checkpoint from {cfg.path.checkpoint_path}")
        checkpoint = torch.load(cfg.path.checkpoint_path, map_location=device)
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„ 'model_state_dict' é”®
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        self.model.load_state_dict(state_dict)
        self.model.to(device)
        self.model.eval()

        self.step_counter = 0
        self.action_queue = [] # ç”¨äº Receding Horizon Control

    def get_txts_bbox_at(self, pos=TARGET_INIT_POS, rot=TARGET_INIT_ROT, scale=TARGET_SCALE):
        anno_path = f"/home/hhhar/liuliu/vld/assets/articulation/{self.cfg.sim.item_id}/link_annotation_gapartnet.json"
        with open(anno_path, "r") as f:
            anno = json.load(f)
            part_num = len(anno)
        target_id = random.randint(0, part_num - 1)
        target_anno = anno[target_id]
        while target_anno["is_gapart"] == False:
            target_id = random.randint(0, part_num - 1)
            target_anno = anno[target_id]
        bbox = target_anno['bbox']
        if target_anno['category'] == "slider_button":
            action_type = 'press'
        else:
            action_type = "rot"

        scaled_bbox = transform_points_by_pose(
            bbox, pos, rot, scale)
        scaled_bbox = np.array(scaled_bbox).reshape(-1, 3)
        self.bbox_info = self.convert_bbox_8points_to_pose(scaled_bbox)
        bbox_info = deepcopy(self.bbox_info)
        del bbox_info["matrix"]  # åˆ é™¤çŸ©é˜µï¼Œä¿ç•™ä¸­å¿ƒã€å°ºå¯¸ã€å››å…ƒæ•°
        return f"action: {action_type}, target_area: {bbox_info}"
    
    def reset(self):
        """é‡ç½® Agent å†…éƒ¨çŠ¶æ€ (æ¸…ç©ºåŠ¨ä½œé˜Ÿåˆ—)"""
        self.action_queue = []
    
    def convert_bbox_8points_to_pose(self, bbox):
        """
        å°† 8 ä¸ªè§’ç‚¹çš„ BBox è½¬æ¢ä¸ºä¸­å¿ƒã€å°ºå¯¸å’Œå››å…ƒæ•°ã€‚

        Args:
            points (np.ndarray): å½¢çŠ¶ä¸º (8, 3) çš„ numpy æ•°ç»„ï¼Œè¡¨ç¤º 8 ä¸ªè§’ç‚¹çš„åæ ‡ã€‚

        Returns:
            dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
                - "center": (3,) np.array, [x, y, z]
                - "extent": (3,) np.array, [length, width, height] (ä¹Ÿç§°ä¸º size)
                - "quat":   (4,) np.array, [w, x, y, z] (Isaac Lab æ ‡å‡†)
                - "matrix": (3, 3) np.array, æ—‹è½¬çŸ©é˜µ
        """
        points = np.array(bbox)

        if points.shape != (8, 3):
            raise ValueError(
                f"Input points must be (8, 3), got {points.shape}")

        # 1. è®¡ç®—ä¸­å¿ƒç‚¹ (Center)
        # å‡ ä½•ä¸­å¿ƒå³æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼
        center = np.mean(points, axis=0)

        # 2. è®¡ç®—æ—‹è½¬çŸ©é˜µ (Rotation)
        # ä½¿ç”¨ PCA/SVD æ–¹æ³•æ‰¾åˆ°ç‰©ä½“çš„ä¸»è½´ã€‚
        # å°†ç‚¹å»ä¸­å¿ƒåŒ–
        centered_points = points - center

        # è®¡ç®—åæ–¹å·®çŸ©é˜µå¹¶è¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ SVD
        # U, S, Vh = SVD(X) -> Vh çš„è¡Œæ˜¯ç‰¹å¾å‘é‡ï¼ˆä¸»è½´æ–¹å‘ï¼‰
        # æ³¨æ„ï¼šPCA å¯¹è½´çš„æ–¹å‘ï¼ˆæ­£è´Ÿï¼‰æœ‰æ¨¡ç³Šæ€§ï¼Œä¸”å¦‚æœç‰©ä½“æ˜¯æ­£æ–¹ä½“ï¼Œè½´å‘å¯èƒ½ä¸ç¨³å®šï¼Œ
        # ä½†å¯¹äºå®šä¹‰ BBox ä¾ç„¶æœ‰æ•ˆã€‚
        u, s, vh = np.linalg.svd(centered_points)

        # æ—‹è½¬çŸ©é˜µçš„åˆ—åº”è¯¥æ˜¯ä¸»è½´æ–¹å‘ï¼Œvh çš„è¡Œæ˜¯ç‰¹å¾å‘é‡
        rotation_matrix = vh.T

        # ã€å…³é”®ä¿®æ­£ã€‘SVD å‡ºæ¥çš„çŸ©é˜µå¯èƒ½å¸¦æœ‰åå°„ï¼ˆè¡Œåˆ—å¼ä¸º -1ï¼‰ï¼Œå³å·¦æ‰‹åæ ‡ç³»
        # æˆ‘ä»¬éœ€è¦å¼ºåˆ¶è½¬æ¢ä¸ºå³æ‰‹åæ ‡ç³»
        if np.linalg.det(rotation_matrix) < 0:
            rotation_matrix[:, 2] *= -1  # åè½¬ Z è½´æ–¹å‘

        # 3. è®¡ç®—å°ºå¯¸ (Dimensions/Extent)
        # å°†åŸå§‹ç‚¹æŠ•å½±åˆ°æ–°çš„ä¸»è½´åæ ‡ç³»ä¸‹ï¼Œè®¡ç®—å„è½´çš„è·¨åº¦
        # projected shape: (8, 3)
        projected_points = centered_points @ rotation_matrix

        min_xyz = np.min(projected_points, axis=0)
        max_xyz = np.max(projected_points, axis=0)

        # å°ºå¯¸ = æœ€å¤§å€¼ - æœ€å°å€¼
        extent = max_xyz - min_xyz

        # 4. è®¡ç®—å››å…ƒæ•° (Quaternion)
        # Isaac Sim/Lab ä½¿ç”¨ [w, x, y, z]
        # Scipy é»˜è®¤è¾“å‡º [x, y, z, w]ï¼Œéœ€è¦è°ƒæ•´é¡ºåº
        r = Rotation.from_matrix(rotation_matrix)
        quat_scipy = r.as_quat()  # [x, y, z, w]
        quat_isaac = np.array(
            [quat_scipy[3], quat_scipy[0], quat_scipy[1], quat_scipy[2]])

        return {
            "center": center.tolist(),
            "extent": extent.tolist(),
            "quat": quat_isaac.tolist(),
            "matrix": rotation_matrix
        }


    def _depth_to_world_pointcloud(self, depth, intrinsic_matrix, camera_pose_4x4, num_points=1024):
        """
        åœ¨æ¨ç†æ—¶å®æ—¶ç”Ÿæˆç‚¹äº‘ã€‚é€»è¾‘éœ€ä¸è®­ç»ƒ Dataset ä¿æŒä¸¥æ ¼ä¸€è‡´ã€‚
        """
        # æ³¨æ„ï¼šè¿™é‡Œè¾“å…¥å·²ç»æ˜¯ GPU Tensor
        H, W = depth.shape
        fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
        cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]

        # 1. ç”Ÿæˆç½‘æ ¼
        v_grid, u_grid = torch.meshgrid(
            torch.arange(H, device=self.device, dtype=torch.float32),
            torch.arange(W, device=self.device, dtype=torch.float32),
            indexing='ij'
        )

        # 2. åæŠ•å½± (Image Plane -> Camera Frame)
        # è¿‡æ»¤æ— æ•ˆæ·±åº¦ (Isaac Sim æœ‰æ—¶è¿”å›æå¤§å€¼æˆ– 0 è¡¨ç¤ºæ— ç©·è¿œ)
        valid_mask = (depth > 0) & (depth < 10.0) 
        z = depth
        x = (u_grid - cx) * z / fx
        y = (v_grid - cy) * z / fy
        
        points_cam = torch.stack([x, y, z], dim=-1).reshape(-1, 3)
        valid_mask_flat = valid_mask.reshape(-1)
        points_cam = points_cam[valid_mask_flat]

        # 3. åæ ‡ç³»ä¿®æ­£ (CV Frame -> Isaac USD Frame)
        # CV: +Z å‰, +Y ä¸‹, +X å³
        # Isaac Camera Prim: -Z å‰, +Y ä¸Š, +X å³
        # ä¿®æ­£å‘é‡: [1, -1, -1]
        correction = torch.tensor([1.0, -1.0, -1.0], device=self.device)
        points_cam = points_cam * correction

        # 4. è½¬ä¸–ç•Œåæ ‡ (Camera Frame -> World Frame)
        # P_world = P_cam @ R.T + T
        R = camera_pose_4x4[:3, :3]
        T = camera_pose_4x4[:3, 3]
        points_world = points_cam @ R.T + T

        # 5. é‡‡æ · (Sampling)
        num_curr = points_world.shape[0]
        if num_curr == 0:
            return torch.zeros((num_points, 3), device=self.device)
            
        if num_curr >= num_points:
            choice = torch.randperm(num_curr, device=self.device)[:num_points]
        else:
            choice = torch.randint(0, num_curr, (num_points,), device=self.device)
        
        return points_world[choice]

    def _dequantize_action(self, action_tokens):
        """
        ç¦»æ•£ Token (0-255) -> å½’ä¸€åŒ–å€¼ (-1, 1) -> ç‰©ç†å€¼ (Pose)
        """
        # 1. Token -> Normalized Value
        # æ‰¾åˆ° bin çš„ä¸­å¿ƒå€¼æˆ–è€…ä¸‹ç•Œ
        # self.bins æ˜¯ linspace(-1, 1, 256)
        # action_tokens shape: (B, Chunk, 7)
        
        # ä½¿ç”¨ embedding lookup çš„æ€æƒ³æˆ–è€…ç›´æ¥ç´¢å¼•
        # ä¸ºäº†ç®€å•ï¼Œç›´æ¥ç”¨ç´¢å¼•æ˜ å°„å› bin çš„å€¼
        # æ³¨æ„: action_tokens æ˜¯ LongTensor
        norm_actions = self.bins[action_tokens] # (B, Chunk, 7)

        # 2. Denormalize
        # x_norm = 2 * (x - min) / (max - min) - 1
        # => x = (x_norm + 1) * (max - min) / 2 + min
        
        min_v = self.min_val.view(1, 1, -1)
        max_v = self.max_val.view(1, 1, -1)
        
        phys_actions = (norm_actions + 1) * (max_v - min_v) / 2 + min_v
        
        # å¼ºåˆ¶å½’ä¸€åŒ–å››å…ƒæ•°
        pos = phys_actions[..., :3]
        quat = phys_actions[..., 3:]
        quat = quat / (torch.norm(quat, dim=-1, keepdim=True) + 1e-8)
        
        phys_actions = torch.cat([pos, quat], dim=-1)
        return phys_actions

    def get_action(self, obs_dict, vis=False):
        """
        ä¸»æ¨ç†å‡½æ•°
        Args:
            obs_dict: åŒ…å« 'rgb', 'depth', 'camera_pose', 'intrinsic', 'instruction'
        Returns:
            next_action: (7,) Tensor, ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„ EE Pose
        """
        # 1. å¤„ç†è¾“å…¥æ•°æ®
        img = obs_dict['rgb'].permute(0, 3, 1, 2).float() / 255.0 # (B, 3, H, W)
        
        # ç”Ÿæˆç‚¹äº‘ (å‡è®¾ batch=1ï¼Œç®€å•å¤„ç† loop)
        pcs = []
        for i in range(img.shape[0]):
            pc = self._depth_to_world_pointcloud(
                obs_dict['depth'][i].squeeze(-1), 
                obs_dict['intrinsic'][i], 
                obs_dict['camera_pose'][i]
            )
            pcs.append(pc)
        pcs = torch.stack(pcs) # (B, 1024, 3)

        raw_texts = obs_dict['instruction'] # List[str]
        text_inputs = self.tokenizer(
            raw_texts,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        for keys in text_inputs:
            text_inputs[keys] = text_inputs[keys].to(self.cfg.sim.device)

        if vis:
            try:
                import open3d as o3d
                print("\nğŸ¨ æ­£åœ¨å¯åŠ¨ Open3D å¯è§†åŒ–çª—å£...")
                print("   (æŒ‰ 'Q' é”®é€€å‡ºçª—å£)")
                
                obb = o3d.geometry.OrientedBoundingBox()
                obb.center = np.array(self.bbox_info["center"])
                obb.extent = np.array(self.bbox_info["extent"])
                obb.R = self.bbox_info["matrix"]
                # è®¾ç½®æ¡†çš„é¢œè‰² (ä¾‹å¦‚çº¢è‰²çº¿æ¡)
                obb.color = (1, 0, 0)

                # è½¬ä¸º Numpy CPU
                pcd_np = pcs[0].cpu().numpy()

                # åˆ›å»º Open3D å¯¹è±¡
                pcd_o3d = o3d.geometry.PointCloud()
                pcd_o3d.points = o3d.utility.Vector3dVector(pcd_np)

                # æ·»åŠ ä¸€ä¸ªåæ ‡è½´ (çº¢X, ç»¿Y, è“Z) ç”¨ä½œä¸–ç•ŒåŸç‚¹å‚è€ƒ
                origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
                    size=1.0, origin=[0, 0, 0])

                # ä¸ºäº†å±•ç¤ºç›¸æœºä½ç½®ï¼Œæˆ‘ä»¬åœ¨ç›¸æœºä½ç½®ä¹Ÿç”»ä¸ªå°åæ ‡è½´
                cam_pos_np = obs_dict['camera_pose'][0][:3, 3].cpu().numpy()
                cam_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
                    size=0.5)
                cam_frame.translate(cam_pos_np)

                o3d.visualization.draw_geometries([pcd_o3d, origin_frame, cam_frame, obb],
                                                    window_name="Isaac Lab World Point Cloud")
            except ImportError:
                print("âš ï¸ æœªå®‰è£… Open3Dï¼Œæ— æ³•è¿›è¡Œç‚¹äº‘å¯è§†åŒ–ã€‚è¯·å®‰è£… open3d åº“åé‡è¯•ã€‚")


        # 2. Receding Horizon Control é€»è¾‘
        # å¦‚æœé˜Ÿåˆ—ç©ºäº†ï¼Œæˆ–è€…éç©ºä½†è¿˜æ²¡æ‰§è¡Œå®Œ horizonï¼Œè¿™é‡Œç®€åŒ–é€»è¾‘ï¼š
        # æ¯æ¬¡éƒ½æ¸…ç©ºé˜Ÿåˆ—é‡æ–°é¢„æµ‹ (Closed Loop)ï¼Œæˆ–è€…ç”¨é˜Ÿåˆ—ç¼“å­˜
        
        execution_horizon = self.cfg.inference.execution_horizon
        # åªè¦é˜Ÿåˆ—é‡Œçš„åŠ¨ä½œå°‘äº horizonï¼Œå°±é‡æ–°æ¨ç†
        if len(self.action_queue) == 0:
            with torch.no_grad():
                # Diffusion æ¨ç†
                # action_tokens: (B, Chunk, 7)
                action_tokens = self.model.predict(img, pcs, text_inputs, num_steps=self.cfg.inference.num_steps)
                
                # åé‡åŒ–
                pred_actions = self._dequantize_action(action_tokens) # (B, Chunk, 7)
                
                # å°†é¢„æµ‹çš„æœªæ¥åŠ¨ä½œå­˜å…¥é˜Ÿåˆ—
                # å‡è®¾ Batch=0 (å•ç¯å¢ƒæµ‹è¯•)
                chunk_len = pred_actions.shape[1]
                # åªå–å‰ execution_horizon æ­¥ï¼Œæˆ–è€…å…¨éƒ¨å–å®Œ
                steps_to_take = min(execution_horizon, chunk_len)
                
                for t in range(steps_to_take):
                    self.action_queue.append(pred_actions[0, t])
        
        # 3. å¼¹å‡ºä¸€ä¸ªåŠ¨ä½œæ‰§è¡Œ
        return self.action_queue.pop(0)

def cal_cammat(pos, tar):
    # ç¡®ä¿è¾“å…¥æ˜¯ Tensor (å¦‚æœä¼ å…¥çš„æ˜¯ list æˆ– numpyï¼Œè¿™é‡Œå…¼å®¹å¤„ç†ä¸€ä¸‹ï¼Œå®é™…ä½¿ç”¨å»ºè®®ç›´æ¥ä¼  Tensor)
    if not isinstance(pos, torch.Tensor):
        pos = torch.tensor(pos)
    if not isinstance(tar, torch.Tensor):
        tar = torch.tensor(tar)
    
    # è·å–è¾“å…¥çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ï¼Œç¡®ä¿åç»­æ–°å»ºçš„ tensor ä¿æŒä¸€è‡´
    device = pos.device
    dtype = pos.dtype

    # 1. è®¡ç®— Z è½´ (Camera Z)
    # ä¹Ÿå°±æ˜¯ä»ç›®æ ‡æŒ‡å‘ç›¸æœº (+Z æŒ‡å‘ç›¸æœºåæ–¹)
    z_axis = pos - tar
    z_axis = z_axis / torch.linalg.norm(z_axis)

    # 2. å®šä¹‰ä¸–ç•Œ ä¸Šæ–¹å‘ (World Up)
    world_up = torch.tensor([0.0, 0.0, 1.0], device=device, dtype=dtype)

    # 3. è®¡ç®— X è½´ (Camera Right)
    # å³ = WorldUp x Z_axis
    x_axis = torch.linalg.cross(world_up, z_axis)

    # æç‚¹ä¿æŠ¤ï¼šå¦‚æœ x_axis æ¨¡é•¿æ¥è¿‘ 0 (è¯´æ˜ç›¸æœºè§†çº¿ä¸ WorldUp å¹³è¡Œ)
    if torch.linalg.norm(x_axis) < 1e-6:
        x_axis = torch.tensor([1.0, 0.0, 0.0], device=device, dtype=dtype)
    else:
        x_axis = x_axis / torch.linalg.norm(x_axis)

    # 4. è®¡ç®— Y è½´ (Camera Up)
    # ä¸Š = Z_axis x X_axis (ä¸¥æ ¼æ­£äº¤)
    y_axis = torch.linalg.cross(z_axis, x_axis)

    # 5. ç»„è£…çŸ©é˜µ
    # åˆ›å»ºå•ä½çŸ©é˜µ
    mat44 = torch.eye(4, device=device, dtype=dtype)
    
    # å¡«å……æ—‹è½¬éƒ¨åˆ† (åˆ—å‘é‡)
    mat44[:3, 0] = x_axis
    mat44[:3, 1] = y_axis
    mat44[:3, 2] = z_axis
    
    # å¡«å……ä½ç§»éƒ¨åˆ†
    mat44[:3, 3] = pos

    return mat44


class EvaluationRunner:
    def __init__(self, cfg, sim, scene, agent, diff_ik_controller, camera):
        self.cfg = cfg
        self.sim = sim
        self.scene = scene
        self.agent = agent
        self.controller = diff_ik_controller
        self.camera = camera
        self.max_steps_per_round = cfg.runner.max_steps_per_round  # æ¯ä¸€è½®æµ‹è¯•æœ€å¤§æ­¥æ•° (ä¾‹å¦‚ 200æ­¥ â‰ˆ 4ç§’æ“ä½œ)
        self.total_rounds = self.cfg.runner.total_rounds          # æ€»å…±æµ‹è¯•å¤šå°‘è½®

        
        # è·å– Robot Entity é…ç½®ç”¨äºé‡ç½®
        self.robot = scene["robot"]
        if args_cli.robot == "franka_panda":
            self.robot_entity_cfg = SceneEntityCfg("robot", joint_names=[".*"], body_names=["panda_hand"])
        else:
             self.robot_entity_cfg = SceneEntityCfg("robot", joint_names=[".*"], body_names=["ee_link"])
        self.robot_entity_cfg.resolve(scene)
        
        # è®¡æ•°å™¨
        self.step_count = 0
        self.round_count = 0
        self.txts = []
        self.goal_marker = None
        self.pt_marker = None

    def reset_env(self):
        """é‡ç½®ç¯å¢ƒã€æœºå™¨äººã€æ§åˆ¶å™¨å’Œ Agent"""
        print(f"[INFO] Resetting environment... (Round {self.round_count})")
        
        # 1. é‡ç½® Robot å…³èŠ‚çŠ¶æ€
        joint_pos = self.robot.data.default_joint_pos.clone()
        joint_vel = self.robot.data.default_joint_vel.clone()
        self.robot.write_joint_state_to_sim(joint_pos, joint_vel)
        self.robot.reset()
        
        # 2. é‡ç½®ç›®æ ‡ç‰©ä½“ (MyNewObject)
        # å¦‚æœéœ€è¦éšæœºä½ç½®ï¼Œå¯ä»¥åœ¨è¿™é‡Œç”Ÿæˆæ–°çš„ Pose å¹¶ write_root_pose_to_sim
        if "my_new_articulation" in self.scene.keys():
            target_obj = self.scene["my_new_articulation"]
            root_state = target_obj.data.default_root_state.clone()
            # ç¤ºä¾‹ï¼šéšæœºåŠ ç‚¹å™ªå£°
            # root_state[:, :3] += torch.randn_like(root_state[:, :3]) * 0.05 
            target_obj.write_root_state_to_sim(root_state)
            target_obj.reset()

        # 3. é‡ç½®æ§åˆ¶å™¨
        self.controller.reset()
        
        # 4. é‡ç½® Agent (æ¸…ç©ºåŠ¨ä½œé˜Ÿåˆ—)
        self.agent.reset()
        
        # 5. é‡ç½®è®¡æ•°å™¨
        self.step_count = 0
        self.round_count += 1

        
        # 6. åˆ·æ–° Simulator
        self.scene.write_data_to_sim()
        self.sim.step()

        self.txts = [self.agent.get_txts_bbox_at()] * self.cfg.sim.num_envs

    def run(self):
        """ä¸»å¾ªç¯"""
        self.reset_env() # åˆå§‹é‡ç½®

        # è®¾ç½®ç›¸æœºçš„å›ºå®šä½å§¿ (Eye-to-hand)
        # ä½ç½®: [0.9, 0.4, 1.0], çœ‹å‘ç›®æ ‡ä¸­å¿ƒ
        camera_positions = torch.tensor([[0.9, 0.4, 1.0]] * self.cfg.sim.num_envs, device=self.cfg.sim.device)
        camera_targets = torch.tensor([TARGET_INIT_POS] * self.cfg.sim.num_envs, device=self.cfg.sim.device)
        self.camera.set_world_poses_from_view(camera_positions, camera_targets)
        cam_mats = [cal_cammat(camera_positions[0], camera_targets[0])] * self.cfg.sim.num_envs

        # Markers
        frame_marker_cfg = FRAME_MARKER_CFG.copy()
        frame_marker_cfg.markers["frame"].scale = (0.1, 0.1, 0.1)
        self.goal_marker = VisualizationMarkers(
            frame_marker_cfg.replace(prim_path="/Visuals/ee_goal"))
        self.pt_marker = VisualizationMarkers(
            bbox_pts_cfg.replace(prim_path="/Visuals/bbox_points"))

        while simulation_app.is_running():
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å¼€å¯æ–°ä¸€è½®
            if self.step_count >= self.max_steps_per_round:
                if self.round_count >= self.total_rounds:
                    print("[INFO] All rounds completed.")
                    break
                self.reset_env()

            # -----------------------------------------------------------
            # 1. è·å–è§‚æµ‹
            # -----------------------------------------------------------
            
            obs = {
                'rgb': self.camera.data.output['rgb'],
                'depth': self.camera.data.output['distance_to_image_plane'],
                'camera_pose': cam_mats, # (B, 4, 4)
                'intrinsic': self.camera.data.intrinsic_matrices,
                'instruction': self.txts
            }


            # -----------------------------------------------------------
            # 2. Agent æ¨ç†
            # -----------------------------------------------------------
            # Target Pose: (7,) [x, y, z, qx, qy, qz, qw]
            target_pose = self.agent.get_action(obs)

            # -----------------------------------------------------------
            # 3. IK æ§åˆ¶
            # -----------------------------------------------------------
            # æ„é€  IK command (Batch=1)
            ik_commands = target_pose.unsqueeze(0) 
            self.controller.set_command(ik_commands)

            self.goal_marker.visualize(
                ik_commands[:, 0:3] + self.scene.env_origins, ik_commands[:, 3:7])
            self.pt_marker.visualize(np.array(self.agent.bbox_info["center"]).reshape(-1, 3))

            # è®¡ç®— Jacobian å’Œå½“å‰çŠ¶æ€
            ee_jacobi_idx = self.robot_entity_cfg.body_ids[0]
            jacobian = self.robot.root_physx_view.get_jacobians()[:, ee_jacobi_idx, :, self.robot_entity_cfg.joint_ids]
            ee_pose_w = self.robot.data.body_pose_w[:, self.robot_entity_cfg.body_ids[0]]
            root_pose_w = self.robot.data.root_pose_w
            joint_pos = self.robot.data.joint_pos[:, self.robot_entity_cfg.joint_ids]

            # ç›¸å¯¹ Pose è®¡ç®—
            ee_pos_b, ee_quat_b = subtract_frame_transforms(
                root_pose_w[:, 0:3], root_pose_w[:, 3:7], 
                ee_pose_w[:, 0:3], ee_pose_w[:, 3:7]
            )
            
            # è®¡ç®—å…³èŠ‚ä½ç½®ç›®æ ‡
            joint_pos_des = self.controller.compute(ee_pos_b, ee_quat_b, jacobian, joint_pos)
            
            # -----------------------------------------------------------
            # 4. æ‰§è¡Œä¸æ›´æ–°
            # -----------------------------------------------------------
            self.robot.set_joint_position_target(joint_pos_des, joint_ids=self.robot_entity_cfg.joint_ids)
            self.scene.write_data_to_sim()
            
            self.sim.step()
            self.scene.update(self.cfg.sim.dt)
            self.camera.update(self.cfg.sim.dt)
            
            self.step_count += 1


# ---------------------------------------------------------------------------
# Main Entry
# ---------------------------------------------------------------------------
@hydra.main(config_path="conf", config_name="eval_config", version_base=None)
def main(cfg: DictConfig):
    # 1. Init Sim
    sim_cfg = sim_utils.SimulationCfg(dt=cfg.sim.dt, device=cfg.sim.device)
    sim = sim_utils.SimulationContext(sim_cfg)
    sim.set_camera_view([1.2, 1.2, 1.2], TARGET_INIT_POS)

    # 2. Scene & Sensor & Controller
    scene_cfg = TableTopSceneCfg(num_envs=cfg.sim.num_envs, env_spacing=2.0)
    scene = InteractiveScene(scene_cfg)
    
    # Camera setup (ensure prim paths match your scene logic)
    for i in range(cfg.sim.num_envs):
        prim_utils.create_prim(f"/World/Origin_{i:02d}", "Xform")

    camera_cfg = CameraCfg(
        prim_path="/World/Origin_.*/CameraSensor",
        update_period=0, height=480, width=640,
        data_types=["rgb", "distance_to_image_plane"],
        spawn=sim_utils.PinholeCameraCfg(
            focal_length=24.0, focus_distance=400.0, 
            horizontal_aperture=20.955, clipping_range=(0.1, 100.0)
        ),
    )
    camera = Camera(cfg=camera_cfg)
    
    diff_ik_cfg = DifferentialIKControllerCfg(command_type="pose", use_relative_mode=False, ik_method="dls")
    diff_ik_controller = DifferentialIKController(diff_ik_cfg, num_envs=scene.num_envs, device=sim.device)

    # 3. Agent
    agent = VLDInferenceAgent(cfg, device=sim.device)

    # 4. Set Fixed Camera Pose (Eye-to-hand)
    sim.reset()
    camera_positions = torch.tensor([[0.9, 0.4, 1.0]] * cfg.sim.num_envs, device=sim.device)
    camera_targets = torch.tensor([TARGET_INIT_POS] * cfg.sim.num_envs, device=sim.device)
    camera.set_world_poses_from_view(camera_positions, camera_targets)

    # 5. Run Evaluation Loop
    runner = EvaluationRunner(cfg, sim, scene, agent, diff_ik_controller, camera)
    runner.run()

if __name__ == "__main__":
    main()
    simulation_app.close()