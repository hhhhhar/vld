from transformers import BertTokenizer
from copy import deepcopy
from IPython import embed
import matplotlib.pyplot as plt
import h5py
import torch
import numpy as np
from scipy.spatial.transform import Rotation
import json
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import unittest
import os
import shutil
import matplotlib
matplotlib.use("Qt5Agg")
import glob
import bisect
import tempfile

intrinsic = np.array([[732.9993,   0.0000, 320.0000],
                      [0.0000, 732.9993, 240.0000],
                      [0.0000,   0.0000,   1.0000]])


class H5RobotDataset(Dataset):
    def __init__(self, h5_dir, action_stats_path, chunk_size=10, num_bins=256, 
                 num_points=1024, vis=False, to_world=False, device='cpu'):
        """
        Args:
            h5_file_path (str): å•ä¸ª H5 æ–‡ä»¶çš„è·¯å¾„ (æˆ–è€…ä½ å¯ä»¥æ”¹ä¸ºæ¥æ”¶æ–‡ä»¶åˆ—è¡¨)
            action_stats_path (str): ä»»åŠ¡ä¸€ç”Ÿæˆçš„ json è·¯å¾„
            num_bins (int): åŠ¨ä½œç¦»æ•£åŒ–çš„æ¡¶æ•°
            num_points (int): ç‚¹äº‘é‡‡æ ·çš„ç‚¹æ•° (PointNet++ éœ€è¦å›ºå®šç‚¹æ•°)
            text_instruction (str): æ—¢ç„¶H5é‡Œæ²¡æœ‰æ–‡æœ¬ï¼Œæˆ‘ä»¬æš‚æ—¶ç”¨ç»Ÿä¸€çš„æŒ‡ä»¤
        """
        self.h5_dir = h5_dir
        self.chunk_size = chunk_size
        self.num_bins = num_bins
        self.num_points = num_points
        self.vis = vis
        self.bbox = []
        self.action_type = []
        self.to_world = to_world
        self.device = device

        # è·å–æ‰€æœ‰ H5 æ–‡ä»¶å¹¶æ’åº (ç¡®ä¿è·¨å¹³å°é¡ºåºä¸€è‡´)
        self.h5_files = sorted(glob.glob(os.path.join(h5_dir, "*.h5")))
        if len(self.h5_files) == 0:
            raise ValueError(f"No .h5 files found in {h5_dir}")

        # åŠ è½½ç»Ÿè®¡é‡
        with open(action_stats_path, 'r') as f:
            stats = json.load(f)
        self.min_val = np.array(stats['min'])
        self.max_val = np.array(stats['max'])
        self.bins = np.linspace(-1, 1, num_bins)

        # å»ºç«‹ç´¢å¼•æ˜ å°„ (Global Index -> File + Local Index)
        # æˆ‘ä»¬éœ€è¦çŸ¥é“æ¯ä¸ªæ–‡ä»¶çš„é•¿åº¦ï¼Œä»¥ä¾¿åœ¨ __getitem__ ä¸­å®šä½
        self.cumulative_sizes = []
        self.total_len = 0
        
        print(f"Indexing {len(self.h5_files)} H5 files...")
        for h5_path in self.h5_files:
            # åªéœ€è¦å¿«é€Ÿè¯»å–ä¸€æ¬¡é•¿åº¦ï¼Œä¸éœ€è¦è¯»å–å…·ä½“æ•°æ®
            with h5py.File(h5_path, 'r') as f:
                seq_len = f['rgb'].shape[0]
                self.total_len += seq_len
                self.cumulative_sizes.append(self.total_len)
        
        print(f"Total frames: {self.total_len}")

    def __len__(self):
        return self.total_len

    def _locate_file(self, idx):
        """
        æ ¹æ®å…¨å±€ index æ‰¾åˆ° (æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å†…å±€éƒ¨ index)
        ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æé«˜æ•ˆç‡
        """
        # æ‰¾åˆ° idx å±äºå“ªä¸ªåŒºé—´
        file_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        
        if file_idx == 0:
            local_idx = idx
        else:
            local_idx = idx - self.cumulative_sizes[file_idx - 1]
            
        return self.h5_files[file_idx], local_idx

    def convert_bbox_8points_to_pose(self):
        """
        å°† 8 ä¸ªè§’ç‚¹çš„ BBox è½¬æ¢ä¸ºä¸­å¿ƒã€å°ºå¯¸å’Œå››å…ƒæ•°ã€‚

        Args:
            points (np.ndarray): å½¢çŠ¶ä¸º (8, 3) çš„ numpy æ•°ç»„ï¼Œè¡¨ç¤º 8 ä¸ªè§’ç‚¹çš„åæ ‡ã€‚

        Returns:
            dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
                - "center": (3,) np.array, [x, y, z]
                - "extent": (3,) np.array, [length, width, height] (ä¹Ÿç§°ä¸º size)
                - "quat":   (4,) np.array, [w, x, y, z] (Isaac Lab æ ‡å‡†)
                - "matrix": (3, 3) np.array, æ—‹è½¬çŸ©é˜µ
        """
        points = np.array(self.bbox)

        if points.shape != (8, 3):
            raise ValueError(
                f"Input points must be (8, 3), got {points.shape}")

        # 1. è®¡ç®—ä¸­å¿ƒç‚¹ (Center)
        # å‡ ä½•ä¸­å¿ƒå³æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼
        center = np.mean(points, axis=0)

        # 2. è®¡ç®—æ—‹è½¬çŸ©é˜µ (Rotation)
        # ä½¿ç”¨ PCA/SVD æ–¹æ³•æ‰¾åˆ°ç‰©ä½“çš„ä¸»è½´ã€‚
        # å°†ç‚¹å»ä¸­å¿ƒåŒ–
        centered_points = points - center

        # è®¡ç®—åæ–¹å·®çŸ©é˜µå¹¶è¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ SVD
        # U, S, Vh = SVD(X) -> Vh çš„è¡Œæ˜¯ç‰¹å¾å‘é‡ï¼ˆä¸»è½´æ–¹å‘ï¼‰
        # æ³¨æ„ï¼šPCA å¯¹è½´çš„æ–¹å‘ï¼ˆæ­£è´Ÿï¼‰æœ‰æ¨¡ç³Šæ€§ï¼Œä¸”å¦‚æœç‰©ä½“æ˜¯æ­£æ–¹ä½“ï¼Œè½´å‘å¯èƒ½ä¸ç¨³å®šï¼Œ
        # ä½†å¯¹äºå®šä¹‰ BBox ä¾ç„¶æœ‰æ•ˆã€‚
        u, s, vh = np.linalg.svd(centered_points)

        # æ—‹è½¬çŸ©é˜µçš„åˆ—åº”è¯¥æ˜¯ä¸»è½´æ–¹å‘ï¼Œvh çš„è¡Œæ˜¯ç‰¹å¾å‘é‡
        rotation_matrix = vh.T

        # ã€å…³é”®ä¿®æ­£ã€‘SVD å‡ºæ¥çš„çŸ©é˜µå¯èƒ½å¸¦æœ‰åå°„ï¼ˆè¡Œåˆ—å¼ä¸º -1ï¼‰ï¼Œå³å·¦æ‰‹åæ ‡ç³»
        # æˆ‘ä»¬éœ€è¦å¼ºåˆ¶è½¬æ¢ä¸ºå³æ‰‹åæ ‡ç³»
        if np.linalg.det(rotation_matrix) < 0:
            rotation_matrix[:, 2] *= -1  # åè½¬ Z è½´æ–¹å‘

        # 3. è®¡ç®—å°ºå¯¸ (Dimensions/Extent)
        # å°†åŸå§‹ç‚¹æŠ•å½±åˆ°æ–°çš„ä¸»è½´åæ ‡ç³»ä¸‹ï¼Œè®¡ç®—å„è½´çš„è·¨åº¦
        # projected shape: (8, 3)
        projected_points = centered_points @ rotation_matrix

        min_xyz = np.min(projected_points, axis=0)
        max_xyz = np.max(projected_points, axis=0)

        # å°ºå¯¸ = æœ€å¤§å€¼ - æœ€å°å€¼
        extent = max_xyz - min_xyz

        # 4. è®¡ç®—å››å…ƒæ•° (Quaternion)
        # Isaac Sim/Lab ä½¿ç”¨ [w, x, y, z]
        # Scipy é»˜è®¤è¾“å‡º [x, y, z, w]ï¼Œéœ€è¦è°ƒæ•´é¡ºåº
        r = Rotation.from_matrix(rotation_matrix)
        quat_scipy = r.as_quat()  # [x, y, z, w]
        quat_isaac = np.array(
            [quat_scipy[3], quat_scipy[0], quat_scipy[1], quat_scipy[2]])

        return {
            "center": center.tolist(),
            "extent": extent.tolist(),
            "quat": quat_isaac.tolist(),
            "matrix": rotation_matrix
        }

    def depth_to_world_pointcloud(self, depth, mask, intrinsic_matrix, camera_pose_4x4):
        """
        å°† Isaac Lab çš„ depth (distance_to_image_plane) è½¬æ¢ä¸ºä¸–ç•Œåæ ‡ç³»ç‚¹äº‘ã€‚

        å‚æ•°:
            depth_tensor (torch.Tensor): (H, W) æ·±åº¦å›¾
            intrinsic_matrix (torch.Tensor): (3, 3) ç›¸æœºå†…å‚ K
            camera_pose_4x4 (torch.Tensor): (4, 4) ç›¸æœºåˆ°ä¸–ç•Œçš„å˜æ¢çŸ©é˜µ (Camera-to-World Pose)
            device (str): è¿è¡Œè®¾å¤‡
            to_world (bool): æ˜¯å¦è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³» (é»˜è®¤ Falseï¼Œåªè¿”å›ç›¸æœºåæ ‡ç³»ä¸‹çš„ç‚¹äº‘)

        è¿”å›:
            torch.Tensor: (N, 3) ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç‚¹äº‘
        """
        # 1. æ•°æ®å‡†å¤‡ä¸è®¾å¤‡ç§»åŠ¨
        cam_mat_np = deepcopy(camera_pose_4x4)
        depth_tensor = torch.from_numpy(depth).float()
        mask_tensor = torch.from_numpy(mask).float()
        intrinsic_matrix = torch.from_numpy(intrinsic_matrix).float()
        camera_pose_4x4 = torch.from_numpy(camera_pose_4x4).float()

        is_valid_mask = torch.any(mask_tensor != 0, dim=-1)
        depth_tensor = depth_tensor * is_valid_mask.float()

        H, W = depth_tensor.shape

        # 2. æå–å†…å‚
        fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
        cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]

        # 3. ç”Ÿæˆåƒç´ ç½‘æ ¼ (u, v)
        # indexing='ij' -> v(è¡Œ/é«˜), u(åˆ—/å®½)
        v_grid, u_grid = torch.meshgrid(
            torch.arange(H, dtype=torch.float32),
            torch.arange(W, dtype=torch.float32),
            indexing='ij'
        )

        # 4. åæŠ•å½± (Back-projection) åˆ°ç›¸æœºåæ ‡ç³»
        # è¿™é‡Œä½¿ç”¨çš„æ˜¯æ ‡å‡†é’ˆå­”ç›¸æœºæ¨¡å‹ (OpenCV Convention)
        # åæ ‡ç³»å®šä¹‰: +Z å‰, +X å³, +Y ä¸‹
        z = depth_tensor
        x = (u_grid - cx) * z / fx
        y = (v_grid - cy) * z / fy

        # å †å ä¸º (N, 3) çŸ©é˜µ
        points_cam_cv = torch.stack([x, y, z], dim=-1).reshape(-1, 3)

        # 5. ã€å…³é”®æ­¥éª¤ã€‘åæ ‡ç³»ä¿®æ­£
        # Isaac Sim çš„ Camera Prim (USD) åæ ‡ç³»å®šä¹‰ä¸º: -Z å‰, +Y ä¸Š, +X å³
        # è€Œä¸Šé¢çš„è®¡ç®—ç»“æœæ˜¯: +Z å‰, +Y ä¸‹, +X å³
        # å¿…é¡»å°†ç‚¹ä» "CV Frame" æ—‹è½¬åˆ° "USD Camera Frame" æ‰èƒ½åº”ç”¨ pose çŸ©é˜µ
        # å˜æ¢é€»è¾‘: x -> x, y -> -y, z -> -z

        correction_vector = torch.tensor([1.0, -1.0, -1.0])
        points = points_cam_cv * correction_vector

        if self.to_world:
            # 6. è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
            # æå–æ—‹è½¬ R (3x3) å’Œ å¹³ç§» T (3)
            R = camera_pose_4x4[:3, :3]
            T = camera_pose_4x4[:3, 3]

            # åº”ç”¨å…¬å¼: P_world = R * P_local + T
            # çŸ©é˜µä¹˜æ³•æ³¨æ„: points æ˜¯ (N, 3), R æ˜¯ (3, 3)
            # çº¿æ€§ä»£æ•°å†™æ³•åº”ä¸º (R @ points.T).T + T
            # ç®€åŒ–ä»£ç å†™æ³•ä¸º points @ R.T + T
            points = points @ R.T + T
        else:
            view_matrix = np.linalg.inv(cam_mat_np)
            R = view_matrix[:3, :3]
            T = view_matrix[:3, 3]
            self.bbox = self.bbox @ R.T + T

        self.bbox = self.convert_bbox_8points_to_pose()

        num_curr = points.shape[0]

        # é™é‡‡æ ·æˆ–è¡¥å…¨åˆ°å›ºå®šç‚¹æ•° (num_points)
        if num_curr >= self.num_points:
            # é™é‡‡æ ·: æ— æ”¾å›é‡‡æ · (replace=False)
            # torch.randperm ç”Ÿæˆ 0 åˆ° N-1 çš„éšæœºæ’åˆ—ï¼Œå–å‰ num_points ä¸ª
            choice = torch.randperm(num_curr, device=points.device)[
                :self.num_points]
            points = points[choice, :]
        else:
            # å¦‚æœç‚¹ä¸å¤Ÿï¼Œéšæœºé‡å¤: æœ‰æ”¾å›é‡‡æ · (replace=True)
            if num_curr > 0:
                # torch.randint ç”Ÿæˆ (0, num_curr) èŒƒå›´å†…çš„éšæœºæ•´æ•°ç´¢å¼•
                choice = torch.randint(
                    0, num_curr, (self.num_points,), device=points.device)
                points = points[choice, :]
            else:
                # æç«¯æƒ…å†µï¼šå…¨é»‘æ·±åº¦å›¾ï¼Œåˆ›å»ºå…¨é›¶ Tensor
                points = torch.zeros((self.num_points, 3),
                                     dtype=points.dtype, device=points.device)

        if self.vis:
            try:
                import open3d as o3d
                print("\nğŸ¨ æ­£åœ¨å¯åŠ¨ Open3D å¯è§†åŒ–çª—å£...")
                print("   (æŒ‰ 'Q' é”®é€€å‡ºçª—å£)")
                
                obb = o3d.geometry.OrientedBoundingBox()
                obb.center = self.bbox["center"]
                obb.extent = self.bbox["extent"]
                obb.R = self.bbox["matrix"]
                
                # è®¾ç½®æ¡†çš„é¢œè‰² (ä¾‹å¦‚çº¢è‰²çº¿æ¡)
                obb.color = (1, 0, 0)
                # è½¬ä¸º Numpy CPU
                pcd_np = points.cpu().numpy()

                # åˆ›å»º Open3D å¯¹è±¡
                pcd_o3d = o3d.geometry.PointCloud()
                pcd_o3d.points = o3d.utility.Vector3dVector(pcd_np)

                # æ·»åŠ ä¸€ä¸ªåæ ‡è½´ (çº¢X, ç»¿Y, è“Z) ç”¨ä½œä¸–ç•ŒåŸç‚¹å‚è€ƒ
                origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
                    size=1.0, origin=[0, 0, 0])

                # ä¸ºäº†å±•ç¤ºç›¸æœºä½ç½®ï¼Œæˆ‘ä»¬åœ¨ç›¸æœºä½ç½®ä¹Ÿç”»ä¸ªå°åæ ‡è½´
                cam_pos_np = camera_pose_4x4[:3, 3].cpu().numpy()
                cam_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
                    size=0.5)
                cam_frame.translate(cam_pos_np)

                o3d.visualization.draw_geometries([pcd_o3d, origin_frame, cam_frame, obb],
                                                  window_name="Isaac Lab World Point Cloud")
            except ImportError:
                print("âš ï¸ æœªå®‰è£… Open3Dï¼Œæ— æ³•è¿›è¡Œç‚¹äº‘å¯è§†åŒ–ã€‚è¯·å®‰è£… open3d åº“åé‡è¯•ã€‚")

        return points

    def _discretize_action(self, action):
        """è¿ç»­åŠ¨ä½œ -> ç¦»æ•£ Token"""
        # 1. Normalize [-1, 1]
        norm_action = 2 * (action - self.min_val) / \
            (self.max_val - self.min_val + 1e-8) - 1
        norm_action = np.clip(norm_action, -1, 1)
        # 2. Binning
        idxs = np.digitize(norm_action, self.bins) - 1
        return np.clip(idxs, 0, self.num_bins - 1)

    def __getitem__(self, idx):
        # åœ¨è¿™é‡Œæ‰“å¼€æ–‡ä»¶ä»¥æ”¯æŒå¤šçº¿ç¨‹ DataLoader
        file_path, local_idx = self._locate_file(idx)

        with h5py.File(file_path, 'r') as f:
            self.bbox = np.array(f["scaled_bbox"][()]).reshape(8, 3)
            self.seq_len = f['rgb'].shape[0]  # é€šå¸¸æ˜¯ 30
            self.action_type = f["action_type"][()].decode('utf-8')

            
            # 1. è¯»å– RGB
            # shape: (480, 640, 3) -> è½¬ä¸º (3, 480, 640) å¹¶å½’ä¸€åŒ–
            rgb = f['rgb'][local_idx]  # uint8
            if self.vis:
                plt.figure(figsize=(10, 6))
                plt.imshow(rgb)
                plt.show()

            image_tensor = torch.from_numpy(
                rgb).permute(2, 0, 1).float() / 255.0

            # 2. è¯»å–æ·±åº¦å¹¶ç”Ÿæˆç‚¹äº‘
            # H5 shape: (30, 480, 640, 1) -> å– [local_idx, :, :, 0]
            depth = f['distance_to_image_plane'][local_idx, :, :, 0]
            mask = f["semantic_segmentation"][local_idx, :, :, :3]
            cam_mat = f['cam_mat'][:]
            pc_tensor = self.depth_to_world_pointcloud(
                depth, mask, intrinsic, cam_mat)  # (N, 3)

            # 3. è¯»å–åŠ¨ä½œ
             # æˆ‘ä»¬éœ€è¦ä» local_idx å¼€å§‹ï¼Œå¾€åè¯»å– chunk_size å¸§
            end_idx = local_idx + self.chunk_size
            
            # f['poses'] shape: (Seq_Len, 1, 7)
            
            if end_idx <= self.seq_len:
                # æƒ…å†µ A: å‰©ä½™é•¿åº¦è¶³å¤Ÿï¼Œç›´æ¥åˆ‡ç‰‡
                # å–å‡º (Chunk, 1, 7) -> Squeeze -> (Chunk, 7)
                raw_actions_chunk = f['poses'][local_idx : end_idx, 0, :]
            else:
                # æƒ…å†µ B: æ¥è¿‘è§†é¢‘æœ«å°¾ï¼Œé•¿åº¦ä¸å¤Ÿ -> éœ€è¦ Padding
                # ç­–ç•¥: å¤åˆ¶æœ€åä¸€å¸§çš„åŠ¨ä½œæ¥å¡«å……
                
                # 1. å…ˆè¯»å‡ºæ‰€æœ‰å‰©ä¸‹çš„æœ‰æ•ˆå¸§
                valid_len = self.seq_len - local_idx
                if valid_len > 0:
                    valid_actions = f['poses'][local_idx : self.seq_len, 0, :] # (Valid, 7)
                    last_action = valid_actions[-1]
                else:
                    # æç«¯æƒ…å†µ (è™½ç„¶ç†è®ºä¸Šä¸ä¼šè¿›è¿™é‡Œ)ï¼Œå…¨éƒ¨ç”¨ 0 å¡«å……
                    valid_actions = np.zeros((0, 7))
                    last_action = np.zeros(7) # æˆ–è€…ä¸Šä¸€å¸§åŠ¨ä½œ

                # 2. æ„é€  Padding
                pad_len = self.chunk_size - valid_len
                pad_block = np.tile(last_action, (pad_len, 1)) # é‡å¤æœ€åä¸€å¸§
                
                # 3. æ‹¼æ¥
                raw_actions_chunk = np.concatenate([valid_actions, pad_block], axis=0)

            # ç¦»æ•£åŒ–
            action_tokens = self._discretize_action(raw_actions_chunk)
            action_tensor = torch.LongTensor(action_tokens) # (Chunk, 7)

        # 4. æ–‡æœ¬ (æ„é€  BERT éœ€è¦çš„è¾“å…¥ï¼Œè¿™é‡Œåªæ˜¯è¿”å› raw textï¼Œéœ€è¦åœ¨ collate_fn é‡Œ tokenize)
        bbox_info = deepcopy(self.bbox)
        del bbox_info["matrix"]  # åˆ é™¤çŸ©é˜µï¼Œä¿ç•™ä¸­å¿ƒã€å°ºå¯¸ã€å››å…ƒæ•°
        text = f"action: {self.action_type}, target_area: {bbox_info}"

        return {
            "image": image_tensor,
            "point_cloud": pc_tensor,
            "text": text,  # åŸå§‹æ–‡æœ¬ï¼Œç¨åç”± Tokenizer å¤„ç†
            "action_tokens": action_tensor
        }


# ================================
# DataLoader çš„ Collate Function
# ================================
# å› ä¸º BERT Tokenizer ä¸èƒ½åœ¨ Dataset é‡Œç›´æ¥åšï¼ˆæ•ˆç‡ä½ï¼‰ï¼Œé€šå¸¸åœ¨ Batch ç»„è£…æ—¶åš


class VLDCollator:
    def __init__(self, tokenizer_name='bert-base-uncased'):
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)

    def __call__(self, batch):
        images = torch.stack([item['image'] for item in batch])
        point_clouds = torch.stack([item['point_cloud'] for item in batch])
        action_tokens = torch.stack([item['action_tokens'] for item in batch])

        # å¤„ç†æ–‡æœ¬ Batch Tokenization
        raw_texts = [item['text'] for item in batch]
        text_inputs = self.tokenizer(
            raw_texts,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )

        return images, point_clouds, text_inputs, action_tokens


class TestH5RobotDatasetChunked(unittest.TestCase):
    def setUp(self):
        """
        æµ‹è¯•å‰çš„å‡†å¤‡å·¥ä½œï¼š
        1. åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹
        2. ç”Ÿæˆå‡çš„ action_stats.json
        3. ç”Ÿæˆå‡çš„ .h5 æ•°æ®æ–‡ä»¶ (åŒ…å« RGB, Depth, Pose ç­‰)
        """
        # 1. åˆ›å»ºä¸´æ—¶ç›®å½•
        self.test_dir = tempfile.mkdtemp()
        self.h5_dir = os.path.join(self.test_dir, "data")
        os.makedirs(self.h5_dir, exist_ok=True)
        self.stats_path = os.path.join(self.test_dir, "action_stats.json")

        # 2. åˆ›å»º Dummy action_stats.json
        stats = {
            "min": [-1.0] * 7,
            "max": [1.0] * 7
        }
        with open(self.stats_path, 'w') as f:
            json.dump(stats, f)

        # 3. åˆ›å»º Dummy H5 æ–‡ä»¶
        # æ¨¡æ‹Ÿä¸€ä¸ªé•¿åº¦ä¸º 30 å¸§çš„è§†é¢‘åºåˆ—
        self.seq_len = 30
        self.chunk_size = 10
        self.n_joints = 7
        self.h5_path = os.path.join(self.h5_dir, "test_01.h5")
        
        with h5py.File(self.h5_path, 'w') as f:
            # RGB: (Seq, H, W, 3) - çº¯é»‘å›¾ç‰‡
            f.create_dataset("rgb", data=np.zeros((self.seq_len, 480, 640, 3), dtype=np.uint8))
            
            # Depth: (Seq, H, W, 1) - å…¨ä¸º 1.0 çš„æ·±åº¦
            f.create_dataset("distance_to_image_plane", data=np.ones((self.seq_len, 480, 640, 1), dtype=np.float32))
            
            # Seg Mask: (Seq, H, W, 3) - å…¨ 1 è¡¨ç¤ºå…¨æ˜¯æœ‰æ•ˆåƒç´ 
            f.create_dataset("semantic_segmentation", data=np.ones((self.seq_len, 480, 640, 3), dtype=np.uint8))
            
            # Cam Mat (ç›¸æœºå†…å‚çŸ©é˜µå ä½)
            f.create_dataset("cam_mat", data=np.eye(4, dtype=np.float32))
            
            # Poses (Actions): (Seq, 1, 7) 
            # æ„é€ æœ‰è§„å¾‹çš„æ•°æ®ï¼šç¬¬ i å¸§çš„æ‰€æœ‰å…³èŠ‚å€¼éƒ½æ˜¯ i/100
            # è¿™æ ·æˆ‘ä»¬åœ¨æµ‹è¯•æ—¶å¯ä»¥é€šè¿‡æ•°å€¼åˆ¤æ–­å®ƒæ˜¯å“ªä¸€å¸§
            poses = np.zeros((self.seq_len, 1, self.n_joints), dtype=np.float32)
            for i in range(self.seq_len):
                poses[i, :, :] = float(i) / 100.0 
            f.create_dataset("poses", data=poses)
            
            # Metadata
            f.create_dataset("action_type", data=np.string_("pick_test"))
            # BBox (1, 8, 3)
            f.create_dataset("scaled_bbox", data=np.zeros((1, 8, 3), dtype=np.float32))

        # 4. åˆå§‹åŒ– Dataset å®ä¾‹
        print(f"\n[SetUp] Initializing Dataset with chunk_size={self.chunk_size}...")
        self.dataset = H5RobotDataset(
            h5_dir=self.h5_dir,
            action_stats_path=self.stats_path,
            chunk_size=self.chunk_size,
            num_bins=256,
            num_points=1024,
            vis=False,
            to_world=False,
            device='cpu' # æ˜¾å¼æµ‹è¯• CPU æ¨¡å¼
        )

    def tearDown(self):
        """æµ‹è¯•ç»“æŸåçš„æ¸…ç†ï¼šåˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹"""
        shutil.rmtree(self.test_dir)

    def test_01_len(self):
        """æµ‹è¯• 1: æ•°æ®é›†æ€»é•¿åº¦æ˜¯å¦æ­£ç¡®"""
        self.assertEqual(len(self.dataset), self.seq_len, 
                         f"Dataset length should be {self.seq_len}")

    def test_02_getitem_shapes(self):
        """æµ‹è¯• 2: è¿”å›æ•°æ®çš„å¼ é‡å½¢çŠ¶æ˜¯å¦ç¬¦åˆ Diffusion è¦æ±‚"""
        idx = 0
        data = self.dataset[idx]
        
        # Image
        self.assertEqual(data['image'].shape, (3, 480, 640))
        self.assertTrue(isinstance(data['image'], torch.Tensor))
        
        # Point Cloud
        self.assertEqual(data['point_cloud'].shape, (1024, 3))
        
        # Action (å…³é”®æ£€æŸ¥ç‚¹)
        # æœŸæœ›å½¢çŠ¶: (Chunk_Size, Joints) å³ (10, 7)
        self.assertEqual(data['action_tokens'].shape, (self.chunk_size, self.n_joints),
                         f"Action shape mismatch! Expected ({self.chunk_size}, 7)")
        self.assertTrue(isinstance(data['action_tokens'], torch.Tensor))
        
        # Text
        self.assertTrue(isinstance(data['text'], str))

    def test_03_padding_logic(self):
        """æµ‹è¯• 3: è¾¹ç•Œæƒ…å†µ (Padding) æ˜¯å¦æ­£ç¡®"""
        # é€‰å–æœ€åä¸€ä¸ªç´¢å¼• (idx = 29)
        # æ­¤æ—¶åªèƒ½è¯»å– 1 å¸§æœ‰æ•ˆæ•°æ®ï¼Œå‰©ä¸‹ 9 å¸§åº”è¯¥ç”± Padding å¡«å……
        idx = self.seq_len - 1
        data = self.dataset[idx]
        
        actions = data['action_tokens'] # shape (10, 7)
        
        # éªŒè¯å½¢çŠ¶
        self.assertEqual(actions.shape, (self.chunk_size, self.n_joints))
        
        # éªŒè¯ Padding å€¼
        # ç¬¬ 0 ä¸ªå…ƒç´ æ˜¯çœŸå®çš„æœ€åä¸€å¸§åŠ¨ä½œ
        # ç¬¬ 1 åˆ° 9 ä¸ªå…ƒç´ åº”è¯¥æ˜¯å¤åˆ¶çš„ç¬¬ 0 ä¸ªå…ƒç´ 
        first_token = actions[0]
        last_token = actions[-1]
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¡Œçš„å€¼éƒ½ç›¸ç­‰ (è¿™æ„å‘³ç€ Padding æˆåŠŸå¤åˆ¶äº†æœ€åä¸€å¸§)
        are_equal = torch.all(actions == first_token)
        self.assertTrue(are_equal, "Padding logic failed: padded frames do not match the last valid frame.")
        print(f"[Test Padding] Index {idx}: Action Chunk returned successfully with padding.")

    def test_04_sliding_window(self):
        """æµ‹è¯• 4: æ­£å¸¸æƒ…å†µä¸‹çš„æ»‘åŠ¨çª—å£è¯»å–"""
        idx = 0
        data = self.dataset[idx]
        actions = data['action_tokens']
        
        # ç”±äºæˆ‘ä»¬é€ çš„æ•°æ®æ˜¯ poses[i] = i/100
        # ç¬¬ 0 ä¸ª chunk åº”è¯¥åŒ…å« pose[0] åˆ° pose[9]
        # å¯¹åº”çš„ token å€¼åº”è¯¥æ˜¯ä¸ä¸€æ ·çš„ (é™¤é num_bins å¤ªå°å¯¼è‡´åˆ†è¾¨ç‡ä¸å¤Ÿï¼Œä½†è¿™é‡Œå‡è®¾è¶³å¤Ÿ)
        
        # ç®€å•çš„æ£€æŸ¥ï¼šç¡®ä¿ chunk é‡Œçš„æ•°æ®ä¸æ˜¯å…¨éƒ¨ä¸€æ ·çš„ (è¯æ˜è¯»åˆ°äº†åºåˆ—)
        # æ³¨æ„ï¼šå¦‚æœ bin åªæœ‰ 1 ä¸ªï¼Œé‚£ç¡®å®ä¼šä¸€æ ·ï¼Œæ‰€ä»¥è¿™é‡Œåªæ˜¯ä¸ªå¼±æ£€æŸ¥
        if self.dataset.num_bins > 10:
            is_all_same = torch.all(actions == actions[0])
            self.assertFalse(is_all_same, "Normal chunking failed: all actions in chunk are identical (expected variation).")

    def test_05_cpu_device_check(self):
        """æµ‹è¯• 5: éªŒè¯ Dataset è¿”å›çš„æ•°æ®ç¡®å®åœ¨ CPU ä¸Š (é˜²æ­¢å¤šè¿›ç¨‹æŠ¥é”™)"""
        idx = 5
        data = self.dataset[idx]
        self.assertEqual(data['image'].device.type, 'cpu')
        self.assertEqual(data['point_cloud'].device.type, 'cpu')
        self.assertEqual(data['action_tokens'].device.type, 'cpu')

    def test_06_dataloader_multiprocessing(self):
        """æµ‹è¯• 6: é›†æˆæµ‹è¯• - æ”¾å…¥ DataLoader å¹¶å¼€å¯å¤šè¿›ç¨‹"""
        print("\n[Test DataLoader] Testing with num_workers=2...")
        
        # collate_fn ç®€å• mock ä¸€ä¸‹ï¼Œå› ä¸º text æ˜¯ stringï¼Œé»˜è®¤ collate å¯èƒ½ä¼šæŠ¥é”™æˆ–è€…å˜æˆ list
        def simple_collate(batch):
            return {
                'image': torch.stack([b['image'] for b in batch]),
                'point_cloud': torch.stack([b['point_cloud'] for b in batch]),
                'action_tokens': torch.stack([b['action_tokens'] for b in batch]),
                'text': [b['text'] for b in batch]
            }

        loader = DataLoader(
            self.dataset, 
            batch_size=4, 
            shuffle=True, 
            num_workers=2, # å¼€å¯å¤šè¿›ç¨‹ï¼è¿™æ˜¯æœ€å®¹æ˜“æŠ¥é”™çš„åœ°æ–¹
            collate_fn=simple_collate
        )
        
        # å°è¯•è¯»å–ä¸€ä¸ª batch
        try:
            for batch in loader:
                # æ£€æŸ¥ Batch ç»´åº¦
                expected_shape = (4, self.chunk_size, self.n_joints)
                self.assertEqual(batch['action_tokens'].shape, expected_shape)
                print("[Test DataLoader] Successfully loaded a batch with multiprocessing.")
                break 
        except RuntimeError as e:
            self.fail(f"DataLoader failed with multiprocessing: {e}")

if __name__ == '__main__':
    # unittest.main()
    h5_path = "/home/hhhar/liuliu/vld/data/regression/data_res/"
    stats_path = "/home/hhhar/liuliu/vld/dataset/action_stats.json"
    dataset = H5RobotDataset(h5_path, stats_path, num_bins=256, vis=True,to_world=True)
    for i in range(len(dataset)):
        item = dataset[i]
        print(
            f"Item {i}: Image {item['image'].shape}, PC {item['point_cloud'].shape}, Action {item['action_tokens']}")
