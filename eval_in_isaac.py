# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# SPDX-License-Identifier: BSD-3-Clause

"""
VLD Diffusion Policy Isaac Lab Evaluation Script.
"""

import os
import argparse


from isaaclab.app import AppLauncher
os.environ["ENABLE_CAMERAS"] = "1"

# 1. å¯åŠ¨ Isaac Sim (å¿…é¡»åœ¨å…¶ä»– import ä¹‹å‰)
parser = argparse.ArgumentParser(description="Evaluate VLD Policy in Isaac Sim.")
parser.add_argument("--robot", type=str, default="franka_panda", help="Name of the robot.")
parser.add_argument("--num_envs", type=int, default=1, help="Number of environments.")
AppLauncher.add_app_launcher_args(parser)
args_cli = parser.parse_args()
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# ---------------------------------------------------------------------------
# Standard Imports
# ---------------------------------------------------------------------------
import torch
import numpy as np
import hydra
import json
import math
from omegaconf import DictConfig
from copy import deepcopy
from IPython import embed
from scipy.spatial.transform import Rotation
import random

# Isaac Lab Imports
import isaaclab.sim as sim_utils
from isaaclab.scene import InteractiveScene, InteractiveSceneCfg
from isaaclab.sensors import Camera, CameraCfg
from isaaclab.assets import AssetBaseCfg, ArticulationCfg
from isaaclab.actuators import ImplicitActuatorCfg
from isaaclab.controllers import DifferentialIKController, DifferentialIKControllerCfg
from isaaclab.managers import SceneEntityCfg
from isaaclab.utils import convert_dict_to_backend
from isaaclab_assets import FRANKA_PANDA_HIGH_PD_CFG, UR10_CFG
from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR
from isaaclab.utils import configclass
from isaaclab.markers.config import FRAME_MARKER_CFG
from isaaclab.markers import VisualizationMarkers, VisualizationMarkersCfg

# VLD Model Imports (å‡è®¾è¿™äº›æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹)
from model.main import VLDDiffusionSystem
from transformers import BertTokenizer

# ---------------------------------------------------------------------------
# Scene Configuration (å¤ç”¨ isaac_render.py çš„é…ç½®)
# ---------------------------------------------------------------------------
TARGET_SCALE = 0.2
TARGET_INIT_POS = (0.5, 0.0, 0.2)
TARGET_INIT_ROT = (0.7, 0.0, 0.7, 0.0)

# è¿™é‡Œä½ éœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ USD è·¯å¾„
MY_NEW_OBJECT_CFG = ArticulationCfg(
    spawn=sim_utils.UsdFileCfg(
        usd_path="/home/hhhar/liuliu/vld/assets/articulation/100392/mobility_annotation_gapartnet/mobility_annotation_gapartnet.usd",
        rigid_props=sim_utils.RigidBodyPropertiesCfg(disable_gravity=False),
        scale=[TARGET_SCALE for _ in range(3)],
        semantic_tags=[("class", "target")],
    ),
    init_state=ArticulationCfg.InitialStateCfg(pos=TARGET_INIT_POS, rot=TARGET_INIT_ROT),
    actuators={
        "all_joints": ImplicitActuatorCfg(
            joint_names_expr=["joint_.*"],
            effort_limit=400.0,
            velocity_limit=100.0,
            stiffness=0.0,
            damping=10.0,
        )
    }
)

bbox_pts_cfg = VisualizationMarkersCfg(
    prim_path="/World/Visuals/testMarkers",
    markers={
        "marker1": sim_utils.SphereCfg(
            radius=0.005,
            visual_material=sim_utils.PreviewSurfaceCfg(
                diffuse_color=(1.0, 0.0, 0.0)),
        ), })

@configclass
class TableTopSceneCfg(InteractiveSceneCfg):
    ground = AssetBaseCfg(
        prim_path="/World/defaultGroundPlane",
        spawn=sim_utils.GroundPlaneCfg(),
        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, -1.05)),
    )
    dome_light = AssetBaseCfg(
        prim_path="/World/Light", spawn=sim_utils.DomeLightCfg(intensity=3000.0, color=(0.75, 0.75, 0.75))
    )
    table = AssetBaseCfg(
        prim_path="{ENV_REGEX_NS}/Table",
        spawn=sim_utils.UsdFileCfg(
            usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Mounts/Stand/stand_instanceable.usd", scale=(2.0, 2.0, 2.0)
        ),
    )
    
    if args_cli.robot == "franka_panda":
        robot = FRANKA_PANDA_HIGH_PD_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
    elif args_cli.robot == "ur10":
        robot = UR10_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
    
    robot.spawn.semantic_tags = [("class", "robot")]
    my_new_articulation = MY_NEW_OBJECT_CFG.replace(prim_path="{ENV_REGEX_NS}/MyNewObject")

def quat_to_rotmat(q):
    """
    q: array-like [w, x, y, z]
    returns 3x3 rotation matrix
    """
    q = np.asarray(q, dtype=float)
    if q.shape != (4,):
        q = q.ravel()[:4]
    # normalize to be safe
    q = q / np.linalg.norm(q)
    w, x, y, z = q
    # 3x3 rotation matrix (Hamilton convention)
    R = np.array([
        [1 - 2*(y*y + z*z),     2*(x*y - z*w),     2*(x*z + y*w)],
        [2*(x*y + z*w), 1 - 2*(x*x + z*z),     2*(y*z - x*w)],
        [2*(x*z - y*w),     2*(y*z + x*w), 1 - 2*(x*x + y*y)]
    ], dtype=float)
    return R

def transform_points_by_pose(points, p, q, scale=1.0):
    """
    points: (N,3) np.array in local coordinates
    p: (3,) translation in world coords
    q: [w,x,y,z] quaternion (world orientation of the local frame)
    scale: scalar or (3,) array, applied in local frame before rotation/translation
    returns transformed_points: (N,3)
    """
    pts = np.asarray(points, dtype=float).reshape(-1, 3)
    # apply local scale (support scalar or per-axis)
    scale = np.asarray(scale, dtype=float)
    if scale.size == 1:
        pts = pts * float(scale)
    else:
        pts = pts * scale.reshape(1, 3)
    # rotate
    R = quat_to_rotmat(q)
    pts_rot = pts.dot(R.T)   # (N,3)
    # translate
    return pts_rot + np.asarray(p).reshape(1, 3)



# ---------------------------------------------------------------------------
# Inference Agent (Core Logic)
# ---------------------------------------------------------------------------
class VLDInferenceAgent:
    def __init__(self, cfg, device):
        self.cfg = cfg
        self.device = device
        self.scaled_bbox = []
        self.action_type = []
        self.bbox_info = []
        
        # 1. åŠ è½½ç»Ÿè®¡é‡ (Min/Max) ç”¨äºåå½’ä¸€åŒ–
        print(f"Loading stats from {cfg.path.action_stats_path}")
        with open(cfg.path.action_stats_path, 'r') as f:
            stats = json.load(f)
        self.min_val = torch.tensor(stats['min'], device=device)
        self.max_val = torch.tensor(stats['max'], device=device)
        self.bins = torch.linspace(-1, 1, cfg.model.num_action_bins, device=device)

        # 2. åˆå§‹åŒ–æ¨¡å‹
        print("Initializing Model...")
        self.model = VLDDiffusionSystem(**cfg.model)
        self.tokenizer = BertTokenizer.from_pretrained(cfg.model.text_model_name)
        
        # 3. åŠ è½½æƒé‡
        print(f"Loading checkpoint from {cfg.path.checkpoint_path}")
        checkpoint = torch.load(cfg.path.checkpoint_path, map_location=device)
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„ 'model_state_dict' é”®
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        self.model.load_state_dict(state_dict)
        self.model.to(device)
        self.model.eval()

        self.step_counter = 0
        self.action_queue = [] # ç”¨äº Receding Horizon Control

    def get_txts_bbox_at(self, pos=TARGET_INIT_POS, rot=TARGET_INIT_ROT, scale=TARGET_SCALE):
        anno_path = f"/home/hhhar/liuliu/vld/assets/articulation/{self.cfg.sim.item_id}/link_annotation_gapartnet.json"
        with open(anno_path, "r") as f:
            anno = json.load(f)
            part_num = len(anno)
        target_id = random.randint(0, part_num - 1)
        target_anno = anno[target_id]
        while target_anno["is_gapart"] == False:
            target_id = random.randint(0, part_num - 1)
            target_anno = anno[target_id]
        bbox = target_anno['bbox']
        if target_anno['category'] == "slider_button":
            action_type = 'press'
        else:
            action_type = "rot"

        scaled_bbox = transform_points_by_pose(
            bbox, pos, rot, scale)
        scaled_bbox = np.array(scaled_bbox).reshape(-1, 3)
        self.bbox_info = self.convert_bbox_8points_to_pose(scaled_bbox)
        bbox_info = deepcopy(self.bbox_info)
        del bbox_info["matrix"]  # åˆ é™¤çŸ©é˜µï¼Œä¿ç•™ä¸­å¿ƒã€å°ºå¯¸ã€å››å…ƒæ•°
        return f"action: {action_type}, target_area: {bbox_info}"

    
    def convert_bbox_8points_to_pose(self, bbox):
        """
        å°† 8 ä¸ªè§’ç‚¹çš„ BBox è½¬æ¢ä¸ºä¸­å¿ƒã€å°ºå¯¸å’Œå››å…ƒæ•°ã€‚

        Args:
            points (np.ndarray): å½¢çŠ¶ä¸º (8, 3) çš„ numpy æ•°ç»„ï¼Œè¡¨ç¤º 8 ä¸ªè§’ç‚¹çš„åæ ‡ã€‚

        Returns:
            dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
                - "center": (3,) np.array, [x, y, z]
                - "extent": (3,) np.array, [length, width, height] (ä¹Ÿç§°ä¸º size)
                - "quat":   (4,) np.array, [w, x, y, z] (Isaac Lab æ ‡å‡†)
                - "matrix": (3, 3) np.array, æ—‹è½¬çŸ©é˜µ
        """
        points = np.array(bbox)

        if points.shape != (8, 3):
            raise ValueError(
                f"Input points must be (8, 3), got {points.shape}")

        # 1. è®¡ç®—ä¸­å¿ƒç‚¹ (Center)
        # å‡ ä½•ä¸­å¿ƒå³æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼
        center = np.mean(points, axis=0)

        # 2. è®¡ç®—æ—‹è½¬çŸ©é˜µ (Rotation)
        # ä½¿ç”¨ PCA/SVD æ–¹æ³•æ‰¾åˆ°ç‰©ä½“çš„ä¸»è½´ã€‚
        # å°†ç‚¹å»ä¸­å¿ƒåŒ–
        centered_points = points - center

        # è®¡ç®—åæ–¹å·®çŸ©é˜µå¹¶è¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ SVD
        # U, S, Vh = SVD(X) -> Vh çš„è¡Œæ˜¯ç‰¹å¾å‘é‡ï¼ˆä¸»è½´æ–¹å‘ï¼‰
        # æ³¨æ„ï¼šPCA å¯¹è½´çš„æ–¹å‘ï¼ˆæ­£è´Ÿï¼‰æœ‰æ¨¡ç³Šæ€§ï¼Œä¸”å¦‚æœç‰©ä½“æ˜¯æ­£æ–¹ä½“ï¼Œè½´å‘å¯èƒ½ä¸ç¨³å®šï¼Œ
        # ä½†å¯¹äºå®šä¹‰ BBox ä¾ç„¶æœ‰æ•ˆã€‚
        u, s, vh = np.linalg.svd(centered_points)

        # æ—‹è½¬çŸ©é˜µçš„åˆ—åº”è¯¥æ˜¯ä¸»è½´æ–¹å‘ï¼Œvh çš„è¡Œæ˜¯ç‰¹å¾å‘é‡
        rotation_matrix = vh.T

        # ã€å…³é”®ä¿®æ­£ã€‘SVD å‡ºæ¥çš„çŸ©é˜µå¯èƒ½å¸¦æœ‰åå°„ï¼ˆè¡Œåˆ—å¼ä¸º -1ï¼‰ï¼Œå³å·¦æ‰‹åæ ‡ç³»
        # æˆ‘ä»¬éœ€è¦å¼ºåˆ¶è½¬æ¢ä¸ºå³æ‰‹åæ ‡ç³»
        if np.linalg.det(rotation_matrix) < 0:
            rotation_matrix[:, 2] *= -1  # åè½¬ Z è½´æ–¹å‘

        # 3. è®¡ç®—å°ºå¯¸ (Dimensions/Extent)
        # å°†åŸå§‹ç‚¹æŠ•å½±åˆ°æ–°çš„ä¸»è½´åæ ‡ç³»ä¸‹ï¼Œè®¡ç®—å„è½´çš„è·¨åº¦
        # projected shape: (8, 3)
        projected_points = centered_points @ rotation_matrix

        min_xyz = np.min(projected_points, axis=0)
        max_xyz = np.max(projected_points, axis=0)

        # å°ºå¯¸ = æœ€å¤§å€¼ - æœ€å°å€¼
        extent = max_xyz - min_xyz

        # 4. è®¡ç®—å››å…ƒæ•° (Quaternion)
        # Isaac Sim/Lab ä½¿ç”¨ [w, x, y, z]
        # Scipy é»˜è®¤è¾“å‡º [x, y, z, w]ï¼Œéœ€è¦è°ƒæ•´é¡ºåº
        r = Rotation.from_matrix(rotation_matrix)
        quat_scipy = r.as_quat()  # [x, y, z, w]
        quat_isaac = np.array(
            [quat_scipy[3], quat_scipy[0], quat_scipy[1], quat_scipy[2]])

        return {
            "center": center.tolist(),
            "extent": extent.tolist(),
            "quat": quat_isaac.tolist(),
            "matrix": rotation_matrix
        }


    def _depth_to_world_pointcloud(self, depth, intrinsic_matrix, camera_pose_4x4, num_points=1024):
        """
        åœ¨æ¨ç†æ—¶å®æ—¶ç”Ÿæˆç‚¹äº‘ã€‚é€»è¾‘éœ€ä¸è®­ç»ƒ Dataset ä¿æŒä¸¥æ ¼ä¸€è‡´ã€‚
        """
        # æ³¨æ„ï¼šè¿™é‡Œè¾“å…¥å·²ç»æ˜¯ GPU Tensor
        H, W = depth.shape
        fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
        cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]

        # 1. ç”Ÿæˆç½‘æ ¼
        v_grid, u_grid = torch.meshgrid(
            torch.arange(H, device=self.device, dtype=torch.float32),
            torch.arange(W, device=self.device, dtype=torch.float32),
            indexing='ij'
        )

        # 2. åæŠ•å½± (Image Plane -> Camera Frame)
        # è¿‡æ»¤æ— æ•ˆæ·±åº¦ (Isaac Sim æœ‰æ—¶è¿”å›æå¤§å€¼æˆ– 0 è¡¨ç¤ºæ— ç©·è¿œ)
        valid_mask = (depth > 0) & (depth < 10.0) 
        z = depth
        x = (u_grid - cx) * z / fx
        y = (v_grid - cy) * z / fy
        
        points_cam = torch.stack([x, y, z], dim=-1).reshape(-1, 3)
        valid_mask_flat = valid_mask.reshape(-1)
        points_cam = points_cam[valid_mask_flat]

        # 3. åæ ‡ç³»ä¿®æ­£ (CV Frame -> Isaac USD Frame)
        # CV: +Z å‰, +Y ä¸‹, +X å³
        # Isaac Camera Prim: -Z å‰, +Y ä¸Š, +X å³
        # ä¿®æ­£å‘é‡: [1, -1, -1]
        correction = torch.tensor([1.0, -1.0, -1.0], device=self.device)
        points_cam = points_cam * correction

        # 4. è½¬ä¸–ç•Œåæ ‡ (Camera Frame -> World Frame)
        # P_world = P_cam @ R.T + T
        R = camera_pose_4x4[:3, :3]
        T = camera_pose_4x4[:3, 3]
        points_world = points_cam @ R.T + T

        # 5. é‡‡æ · (Sampling)
        num_curr = points_world.shape[0]
        if num_curr == 0:
            return torch.zeros((num_points, 3), device=self.device)
            
        if num_curr >= num_points:
            choice = torch.randperm(num_curr, device=self.device)[:num_points]
        else:
            choice = torch.randint(0, num_curr, (num_points,), device=self.device)
        
        return points_world[choice]

    def _dequantize_action(self, action_tokens):
        """
        ç¦»æ•£ Token (0-255) -> å½’ä¸€åŒ–å€¼ (-1, 1) -> ç‰©ç†å€¼ (Pose)
        """
        # 1. Token -> Normalized Value
        # æ‰¾åˆ° bin çš„ä¸­å¿ƒå€¼æˆ–è€…ä¸‹ç•Œ
        # self.bins æ˜¯ linspace(-1, 1, 256)
        # action_tokens shape: (B, Chunk, 7)
        
        # ä½¿ç”¨ embedding lookup çš„æ€æƒ³æˆ–è€…ç›´æ¥ç´¢å¼•
        # ä¸ºäº†ç®€å•ï¼Œç›´æ¥ç”¨ç´¢å¼•æ˜ å°„å› bin çš„å€¼
        # æ³¨æ„: action_tokens æ˜¯ LongTensor
        norm_actions = self.bins[action_tokens] # (B, Chunk, 7)

        # 2. Denormalize
        # x_norm = 2 * (x - min) / (max - min) - 1
        # => x = (x_norm + 1) * (max - min) / 2 + min
        
        min_v = self.min_val.view(1, 1, -1)
        max_v = self.max_val.view(1, 1, -1)
        
        phys_actions = (norm_actions + 1) * (max_v - min_v) / 2 + min_v
        return phys_actions

    def get_action(self, obs_dict, vis=False):
        """
        ä¸»æ¨ç†å‡½æ•°
        Args:
            obs_dict: åŒ…å« 'rgb', 'depth', 'camera_pose', 'intrinsic', 'instruction'
        Returns:
            next_action: (7,) Tensor, ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„ EE Pose
        """
        # 1. å¤„ç†è¾“å…¥æ•°æ®
        img = obs_dict['rgb'].permute(0, 3, 1, 2).float() / 255.0 # (B, 3, H, W)
        
        # ç”Ÿæˆç‚¹äº‘ (å‡è®¾ batch=1ï¼Œç®€å•å¤„ç† loop)
        pcs = []
        for i in range(img.shape[0]):
            pc = self._depth_to_world_pointcloud(
                obs_dict['depth'][i].squeeze(-1), 
                obs_dict['intrinsic'][i], 
                obs_dict['camera_pose'][i]
            )
            pcs.append(pc)
        pcs = torch.stack(pcs) # (B, 1024, 3)

        raw_texts = obs_dict['instruction'] # List[str]
        text_inputs = self.tokenizer(
            raw_texts,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        for keys in text_inputs:
            text_inputs[keys] = text_inputs[keys].to(self.cfg.sim.device)

        if vis:
            try:
                import open3d as o3d
                print("\nğŸ¨ æ­£åœ¨å¯åŠ¨ Open3D å¯è§†åŒ–çª—å£...")
                print("   (æŒ‰ 'Q' é”®é€€å‡ºçª—å£)")
                
                obb = o3d.geometry.OrientedBoundingBox()
                obb.center = np.array(self.bbox_info["center"])
                obb.extent = np.array(self.bbox_info["extent"])
                obb.R = self.bbox_info["matrix"]
                # è®¾ç½®æ¡†çš„é¢œè‰² (ä¾‹å¦‚çº¢è‰²çº¿æ¡)
                obb.color = (1, 0, 0)

                # è½¬ä¸º Numpy CPU
                pcd_np = pcs[0].cpu().numpy()

                # åˆ›å»º Open3D å¯¹è±¡
                pcd_o3d = o3d.geometry.PointCloud()
                pcd_o3d.points = o3d.utility.Vector3dVector(pcd_np)

                # æ·»åŠ ä¸€ä¸ªåæ ‡è½´ (çº¢X, ç»¿Y, è“Z) ç”¨ä½œä¸–ç•ŒåŸç‚¹å‚è€ƒ
                origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
                    size=1.0, origin=[0, 0, 0])

                # ä¸ºäº†å±•ç¤ºç›¸æœºä½ç½®ï¼Œæˆ‘ä»¬åœ¨ç›¸æœºä½ç½®ä¹Ÿç”»ä¸ªå°åæ ‡è½´
                cam_pos_np = obs_dict['camera_pose'][0][:3, 3].cpu().numpy()
                cam_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
                    size=0.5)
                cam_frame.translate(cam_pos_np)

                o3d.visualization.draw_geometries([pcd_o3d, origin_frame, cam_frame, obb],
                                                    window_name="Isaac Lab World Point Cloud")
            except ImportError:
                print("âš ï¸ æœªå®‰è£… Open3Dï¼Œæ— æ³•è¿›è¡Œç‚¹äº‘å¯è§†åŒ–ã€‚è¯·å®‰è£… open3d åº“åé‡è¯•ã€‚")


        # 2. Receding Horizon Control é€»è¾‘
        # å¦‚æœé˜Ÿåˆ—ç©ºäº†ï¼Œæˆ–è€…éç©ºä½†è¿˜æ²¡æ‰§è¡Œå®Œ horizonï¼Œè¿™é‡Œç®€åŒ–é€»è¾‘ï¼š
        # æ¯æ¬¡éƒ½æ¸…ç©ºé˜Ÿåˆ—é‡æ–°é¢„æµ‹ (Closed Loop)ï¼Œæˆ–è€…ç”¨é˜Ÿåˆ—ç¼“å­˜
        
        execution_horizon = self.cfg.inference.execution_horizon
        # åªè¦é˜Ÿåˆ—é‡Œçš„åŠ¨ä½œå°‘äº horizonï¼Œå°±é‡æ–°æ¨ç†
        if len(self.action_queue) == 0:
            with torch.no_grad():
                # Diffusion æ¨ç†
                # action_tokens: (B, Chunk, 7)
                action_tokens = self.model.predict(img, pcs, text_inputs, num_steps=self.cfg.inference.num_steps)
                
                # åé‡åŒ–
                pred_actions = self._dequantize_action(action_tokens) # (B, Chunk, 7)
                
                # å°†é¢„æµ‹çš„æœªæ¥åŠ¨ä½œå­˜å…¥é˜Ÿåˆ—
                # å‡è®¾ Batch=0 (å•ç¯å¢ƒæµ‹è¯•)
                chunk_len = pred_actions.shape[1]
                # åªå–å‰ execution_horizon æ­¥ï¼Œæˆ–è€…å…¨éƒ¨å–å®Œ
                steps_to_take = min(execution_horizon, chunk_len)
                
                for t in range(steps_to_take):
                    self.action_queue.append(pred_actions[0, t])
        
        # 3. å¼¹å‡ºä¸€ä¸ªåŠ¨ä½œæ‰§è¡Œ
        return self.action_queue.pop(0)

def cal_cammat(pos, tar):
    # ç¡®ä¿è¾“å…¥æ˜¯ Tensor (å¦‚æœä¼ å…¥çš„æ˜¯ list æˆ– numpyï¼Œè¿™é‡Œå…¼å®¹å¤„ç†ä¸€ä¸‹ï¼Œå®é™…ä½¿ç”¨å»ºè®®ç›´æ¥ä¼  Tensor)
    if not isinstance(pos, torch.Tensor):
        pos = torch.tensor(pos)
    if not isinstance(tar, torch.Tensor):
        tar = torch.tensor(tar)
    
    # è·å–è¾“å…¥çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ï¼Œç¡®ä¿åç»­æ–°å»ºçš„ tensor ä¿æŒä¸€è‡´
    device = pos.device
    dtype = pos.dtype

    # 1. è®¡ç®— Z è½´ (Camera Z)
    # ä¹Ÿå°±æ˜¯ä»ç›®æ ‡æŒ‡å‘ç›¸æœº (+Z æŒ‡å‘ç›¸æœºåæ–¹)
    z_axis = pos - tar
    z_axis = z_axis / torch.linalg.norm(z_axis)

    # 2. å®šä¹‰ä¸–ç•Œ ä¸Šæ–¹å‘ (World Up)
    world_up = torch.tensor([0.0, 0.0, 1.0], device=device, dtype=dtype)

    # 3. è®¡ç®— X è½´ (Camera Right)
    # å³ = WorldUp x Z_axis
    x_axis = torch.linalg.cross(world_up, z_axis)

    # æç‚¹ä¿æŠ¤ï¼šå¦‚æœ x_axis æ¨¡é•¿æ¥è¿‘ 0 (è¯´æ˜ç›¸æœºè§†çº¿ä¸ WorldUp å¹³è¡Œ)
    if torch.linalg.norm(x_axis) < 1e-6:
        x_axis = torch.tensor([1.0, 0.0, 0.0], device=device, dtype=dtype)
    else:
        x_axis = x_axis / torch.linalg.norm(x_axis)

    # 4. è®¡ç®— Y è½´ (Camera Up)
    # ä¸Š = Z_axis x X_axis (ä¸¥æ ¼æ­£äº¤)
    y_axis = torch.linalg.cross(z_axis, x_axis)

    # 5. ç»„è£…çŸ©é˜µ
    # åˆ›å»ºå•ä½çŸ©é˜µ
    mat44 = torch.eye(4, device=device, dtype=dtype)
    
    # å¡«å……æ—‹è½¬éƒ¨åˆ† (åˆ—å‘é‡)
    mat44[:3, 0] = x_axis
    mat44[:3, 1] = y_axis
    mat44[:3, 2] = z_axis
    
    # å¡«å……ä½ç§»éƒ¨åˆ†æ¯ä¸€å¸§éƒ½æ›´æ–°
    mat44[:3, 3] = pos

    return mat44

# ---------------------------------------------------------------------------
# Main Simulation Loop
# ---------------------------------------------------------------------------
@hydra.main(config_path="conf", config_name="eval_config", version_base=None)
def main(cfg: DictConfig):
    # 1. åˆå§‹åŒ– Simulation
    sim_cfg = sim_utils.SimulationCfg(dt=cfg.sim.dt, device=cfg.sim.device)
    sim = sim_utils.SimulationContext(sim_cfg)
    
    # è®¾ç½®è§†è§’
    sim.set_camera_view([0.9, 0.4, 1.0], TARGET_INIT_POS)

    # 2. æ„å»ºåœºæ™¯
    scene_cfg = TableTopSceneCfg(num_envs=cfg.sim.num_envs, env_spacing=2.0)
    scene = InteractiveScene(scene_cfg)
    
    # 3. è®¾ç½®ä¼ æ„Ÿå™¨ (Camera)
    camera_cfg = CameraCfg(
        prim_path="/World/Origin_.*/CameraSensor",
        update_period=0, # æ¯ä¸€å¸§éƒ½æ›´æ–°
        height=480, width=640,
        data_types=["rgb", "distance_to_image_plane"], # åªéœ€ RGB å’Œ Depth
        spawn=sim_utils.PinholeCameraCfg(
            focal_length=24.0, focus_distance=400.0, 
            horizontal_aperture=20.955, clipping_range=(0.1, 1.0e5)
        ),
    )
    # åˆ›å»ºç›¸æœº prim (éœ€è¦æŒ‚è½½åœ¨æŸä¸ª Xform ä¸‹ï¼Œè¿™é‡Œç®€å•æŒ‚è½½)
    # æ³¨æ„ï¼šæ›´ä¸¥è°¨çš„åšæ³•æ˜¯æŒ‚è½½åœ¨ Robot æ‰‹ä¸Š(Eye-in-hand) æˆ–å›ºå®šä½ç½®(Eye-to-hand)
    # è¿™é‡Œå¤ç”¨ isaac_render.py çš„é€»è¾‘ï¼ŒæŒ‚è½½åœ¨ /World/Origin_XX
    import isaacsim.core.utils.prims as prim_utils
    for i in range(cfg.sim.num_envs):
        prim_utils.create_prim(f"/World/Origin_{i:02d}", "Xform")
    
    camera = Camera(cfg=camera_cfg)

    # 4. è®¾ç½®æ§åˆ¶å™¨ (Differential IK)
    diff_ik_cfg = DifferentialIKControllerCfg(command_type="pose", use_relative_mode=False, ik_method="dls")
    diff_ik_controller = DifferentialIKController(diff_ik_cfg, num_envs=scene.num_envs, device=sim.device)

    # 5. åˆå§‹åŒ–æ¨ç† Agent
    agent = VLDInferenceAgent(cfg, device=sim.device)

    # 6. Reset
    sim.reset()
    print("[INFO]: Simulation ready. Starting inference loop...")

    # è®¾ç½®ç›¸æœºçš„å›ºå®šä½å§¿ (Eye-to-hand)
    # ä½ç½®: [0.9, 0.4, 1.0], çœ‹å‘ç›®æ ‡ä¸­å¿ƒ
    camera_positions = torch.tensor([[0.9, 0.4, 1.0]] * cfg.sim.num_envs, device=sim.device)
    camera_targets = torch.tensor([TARGET_INIT_POS] * cfg.sim.num_envs, device=sim.device)
    camera.set_world_poses_from_view(camera_positions, camera_targets)
    cam_mats = [cal_cammat(camera_positions[0], camera_targets[0])] * cfg.sim.num_envs

    # è·å– Robot Entity
    robot = scene["robot"]
    robot_entity_cfg = SceneEntityCfg("robot", joint_names=[".*"], body_names=["panda_hand"]) # Franka EE name
    robot_entity_cfg.resolve(scene)

    frame_marker_cfg = FRAME_MARKER_CFG.copy()
    frame_marker_cfg.markers["frame"].scale = (0.1, 0.1, 0.1)
    goal_marker = VisualizationMarkers(
        frame_marker_cfg.replace(prim_path="/Visuals/ee_goal"))
    pt_marker = VisualizationMarkers(
        bbox_pts_cfg.replace(prim_path="/Visuals/bbox_points"))


    sim.step()
    
    # æ›´æ–°ä¼ æ„Ÿå™¨ & åœºæ™¯
    scene.update(cfg.sim.dt)
    camera.update(cfg.sim.dt)
    
    while simulation_app.is_running():
        # 1. è·å–è§‚æµ‹æ•°æ®
        # å¿…é¡»å…ˆ camera.update(dt) æ‰èƒ½æ‹¿åˆ°æœ€æ–°æ•°æ®ï¼Œä½† Camera ç±»é€šå¸¸åœ¨ scene.update åè‡ªåŠ¨å¤„ç†
        # æ˜¾å¼è°ƒç”¨ä»¥ç¡®ä¿æ•°æ®åŒæ­¥
        # camera.update(dt=sim.get_physics_dt()) 
        
        # ç»„ç»‡æ•°æ®
        # convert_dict_to_backend ä¼šå¤„ç† backend è½¬æ¢ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥è¦ Tensor
        txts = [agent.get_txts_bbox_at()] * cfg.sim.num_envs
        obs = {
            'rgb': camera.data.output['rgb'], # (B, H, W, 4)
            'depth': camera.data.output['distance_to_image_plane'], # (B, H, W, 1)
            'camera_pose': cam_mats, # (B, 4, 4)
            'intrinsic': camera.data.intrinsic_matrices, # (B, 3, 3)
            'instruction': txts
        }

        # 2. Agent æ¨ç†
        # action shape: (7,) -> [x, y, z, qx, qy, qz, qw]
        target_pose = agent.get_action(obs)

                
        # 3. æ§åˆ¶å™¨è®¡ç®—
        # IK Controller éœ€è¦:
        # - ee_pos_b, ee_quat_b (ç›¸å¯¹äº Base çš„ EE Pose)
        # - jacobian
        # - joint_pos
        
        # è·å– Robot å½“å‰çŠ¶æ€
        # Isaac Lab çš„ IK Controller å…¶å®å¯ä»¥ç›´æ¥æ¥æ”¶ set_command (Target Pose in Root Frame or Relative)
        # å¦‚æœ command_type="pose" ä¸” use_relative_mode=Falseï¼Œcommand åº”è¯¥æ˜¯ç›®æ ‡ Pose (absolute)
        
        # æ„é€  IK command
        # command shape: (B, 7)
        ik_commands = target_pose.unsqueeze(0) # Batch=1

        #Markers
        goal_marker.visualize(
            ik_commands[:, 0:3] + scene.env_origins, ik_commands[:, 3:7])
        
        # è®¾ç½®ç›®æ ‡
        diff_ik_controller.set_command(ik_commands)
        
        # è®¡ç®—å…³èŠ‚æŒ‡ä»¤
        # éœ€è¦æ‰‹åŠ¨è®¡ç®— Jacobian ç­‰? ä¸ï¼ŒIsaac Lab çš„ Controller å°è£…äº† compute æ–¹æ³•
        # æˆ‘ä»¬éœ€è¦æŒ‰ç…§ isaac_render.py é‡Œçš„å†™æ³•:
        
        ee_jacobi_idx = robot_entity_cfg.body_ids[0]
        jacobian = robot.root_physx_view.get_jacobians()[:, ee_jacobi_idx, :, robot_entity_cfg.joint_ids]
        ee_pose_w = robot.data.body_pose_w[:, robot_entity_cfg.body_ids[0]]
        root_pose_w = robot.data.root_pose_w
        joint_pos = robot.data.joint_pos[:, robot_entity_cfg.joint_ids]
        
        # è®¡ç®—ç›¸å¯¹ Pose (Root Frame)
        from isaaclab.utils.math import subtract_frame_transforms
        ee_pos_b, ee_quat_b = subtract_frame_transforms(
            root_pose_w[:, 0:3], root_pose_w[:, 3:7], 
            ee_pose_w[:, 0:3], ee_pose_w[:, 3:7]
        )
        
        # è®¡ç®—å…³èŠ‚ä½ç½®
        joint_pos_des = diff_ik_controller.compute(ee_pos_b, ee_quat_b, jacobian, joint_pos)
        
        # 4. æ‰§è¡ŒåŠ¨ä½œ
        robot.set_joint_position_target(joint_pos_des, joint_ids=robot_entity_cfg.joint_ids)
        scene.write_data_to_sim()
        
        sim.step()
        
        # æ›´æ–°ä¼ æ„Ÿå™¨ & åœºæ™¯
        scene.update(cfg.sim.dt)
        camera.update(cfg.sim.dt)

if __name__ == "__main__":
    main()
    simulation_app.close()


